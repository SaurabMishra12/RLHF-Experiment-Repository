{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNKCtjBWT93WlGFPRD2Uqxl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["\n","!pip install python-dotenv\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"81lU1PSSRV1I","executionInfo":{"status":"ok","timestamp":1725558013107,"user_tz":-330,"elapsed":4430,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}},"outputId":"e22f4860-53af-49ad-a3ff-5943439e1c99"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-dotenv\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Installing collected packages: python-dotenv\n","Successfully installed python-dotenv-1.0.1\n"]}]},{"cell_type":"code","source":["import os\n","from dotenv import load_dotenv\n","import json\n","import base64\n","from google.auth.transport.requests import Request\n","from google.oauth2.service_account import Credentials\n","\n","def authenticate():\n","    #Load .env\n","    load_dotenv()\n","    #DLAI Custom Key\n","    return \"DLAI_CREDENTIALS\", \"DLAI_PROJECT\", \"gs://gcp-sc2-rlhf\"\n","\n","    #Decode key and store in .JSON\n","    SERVICE_ACCOUNT_KEY_STRING_B64 = os.getenv('SERVICE_ACCOUNT_KEY')\n","    SERVICE_ACCOUNT_KEY_BYTES_B64 = SERVICE_ACCOUNT_KEY_STRING_B64.encode(\"ascii\")\n","    SERVICE_ACCOUNT_KEY_STRING_BYTES = base64.b64decode(SERVICE_ACCOUNT_KEY_BYTES_B64)\n","    SERVICE_ACCOUNT_KEY_STRING = SERVICE_ACCOUNT_KEY_STRING_BYTES.decode(\"ascii\")\n","\n","    SERVICE_ACCOUNT_KEY = json.loads(SERVICE_ACCOUNT_KEY_STRING)\n","\n","\n","    # Create credentials based on key from service account\n","    # Make sure your account has the roles listed in the Google Cloud Setup section\n","    credentials = Credentials.from_service_account_info(\n","        SERVICE_ACCOUNT_KEY,\n","        scopes=['https://www.googleapis.com/auth/cloud-platform'])\n","\n","    if credentials.expired:\n","        credentials.refresh(Request())\n","\n","    #Set project ID according to environment variable\n","    PROJECT_ID = os.getenv('PROJECT_ID')\n","    STAGING_BUCKET = os.getenv('STAGING_BUCKET')# 'gs://gcp-sc2-rlhf-staging'\n","\n","    return credentials, PROJECT_ID, STAGING_BUCKET\n"],"metadata":{"id":"LN5LKlB_O-Pr","executionInfo":{"status":"ok","timestamp":1725558029295,"user_tz":-330,"elapsed":467,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hRGNJyb7t2ww","executionInfo":{"status":"ok","timestamp":1725556073353,"user_tz":-330,"elapsed":26435,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}},"outputId":"e8160df6-ebcf-4e5f-8623-4ee506e55500"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting google-cloud-pipeline-components\n","  Downloading google_cloud_pipeline_components-2.16.1-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pipeline-components) (2.19.2)\n","Collecting kfp<=2.7.0,>=2.6.0 (from google-cloud-pipeline-components)\n","  Downloading kfp-2.7.0.tar.gz (441 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/441.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m440.3/441.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.8/441.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: google-cloud-aiplatform<2,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pipeline-components) (1.64.0)\n","Requirement already satisfied: Jinja2<4,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pipeline-components) (3.1.4)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (1.65.0)\n","Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (3.20.3)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (1.24.0)\n","Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (2.27.0)\n","Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (2.32.3)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (24.1)\n","Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.8.0)\n","Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (3.25.0)\n","Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (1.12.5)\n","Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.0.6)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.8.2)\n","Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (0.16)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=3.1.2->google-cloud-pipeline-components) (2.1.5)\n","Requirement already satisfied: click<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (8.1.7)\n","Collecting kfp-pipeline-spec==0.3.0 (from kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components)\n","  Downloading kfp_pipeline_spec-0.3.0-py3-none-any.whl.metadata (329 bytes)\n","Collecting kfp-server-api<2.1.0,>=2.0.0 (from kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components)\n","  Downloading kfp-server-api-2.0.5.tar.gz (63 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting kubernetes<27,>=8.0.0 (from kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components)\n","  Downloading kubernetes-26.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components)\n","  Downloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Requirement already satisfied: PyYAML<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (6.0.2)\n","Collecting requests-toolbelt<1,>=0.8.0 (from kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components)\n","  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (0.9.0)\n","Collecting urllib3<2.0.0 (from kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components)\n","  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (1.64.1)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (1.48.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (4.9)\n","Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.4.1)\n","Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.7.2)\n","Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.8.2)\n","Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (0.13.1)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (1.16.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (2024.8.30)\n","Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (71.0.4)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (1.8.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (1.3.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.20.1)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (3.8)\n","Requirement already satisfied: numpy<3,>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (1.26.4)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (1.5.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (3.2.2)\n","Downloading google_cloud_pipeline_components-2.16.1-py3-none-any.whl (1.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading kfp_pipeline_spec-0.3.0-py3-none-any.whl (12 kB)\n","Downloading kubernetes-26.1.0-py2.py3-none-any.whl (1.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: kfp, kfp-server-api\n","  Building wheel for kfp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kfp: filename=kfp-2.7.0-py3-none-any.whl size=610411 sha256=9822391877530453247fdf2f53b82ab0632a64d3efc19c69033f5b184c7c5a3b\n","  Stored in directory: /root/.cache/pip/wheels/9e/7d/a4/f9d013e82681c9746ef10de3b00456163577a99279c5ed673d\n","  Building wheel for kfp-server-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kfp-server-api: filename=kfp_server_api-2.0.5-py3-none-any.whl size=114735 sha256=f40753d40f912f52000bca634272a3335b122c3d7aeebbe026e5a539bd55c54c\n","  Stored in directory: /root/.cache/pip/wheels/ac/4f/f0/2f622aadcbf8921fb72d24f52efaffacc235f863c195c289c5\n","Successfully built kfp kfp-server-api\n","Installing collected packages: urllib3, protobuf, kfp-server-api, kfp-pipeline-spec, requests-toolbelt, kubernetes, kfp, google-cloud-pipeline-components\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.0.7\n","    Uninstalling urllib3-2.0.7:\n","      Successfully uninstalled urllib3-2.0.7\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed google-cloud-pipeline-components-2.16.1 kfp-2.7.0 kfp-pipeline-spec-0.3.0 kfp-server-api-2.0.5 kubernetes-26.1.0 protobuf-4.25.4 requests-toolbelt-0.10.1 urllib3-1.26.20\n","Requirement already satisfied: kfp in /usr/local/lib/python3.10/dist-packages (2.7.0)\n","Requirement already satisfied: click<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (8.1.7)\n","Requirement already satisfied: docstring-parser<1,>=0.7.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.16)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.19.2)\n","Requirement already satisfied: google-auth<3,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.27.0)\n","Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.8.0)\n","Requirement already satisfied: kfp-pipeline-spec==0.3.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.3.0)\n","Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.0.5)\n","Requirement already satisfied: kubernetes<27,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (26.1.0)\n","Requirement already satisfied: protobuf<5,>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (4.25.4)\n","Requirement already satisfied: PyYAML<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (6.0.2)\n","Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.10.1)\n","Requirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.9.0)\n","Requirement already satisfied: urllib3<2.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (1.26.20)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.65.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.24.0)\n","Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.32.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n","Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.4.1)\n","Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.7.2)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (1.16.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (2024.8.30)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (2.8.2)\n","Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp) (71.0.4)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp) (1.8.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp) (1.3.1)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3,>=2.2.1->kfp) (1.5.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.6.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp) (3.2.2)\n"]}],"source":["!pip3 install google-cloud-pipeline-components\n","!pip3 install kfp\n"]},{"cell_type":"code","source":["# Import (RLFH is currently in preview)\n","from google_cloud_pipeline_components.preview.llm \\\n","import rlhf_pipeline"],"metadata":{"id":"Z9zXNj0Vn8_b","executionInfo":{"status":"ok","timestamp":1725556074142,"user_tz":-330,"elapsed":792,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Import from KubeFlow pipelines\n","from kfp import compiler"],"metadata":{"id":"CXsX_28qJ0Ez","executionInfo":{"status":"ok","timestamp":1725556080526,"user_tz":-330,"elapsed":459,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Define a path to the yaml file\n","RLHF_PIPELINE_PKG_PATH = \"rlhf_pipeline.yaml\""],"metadata":{"id":"go2-Vc9mKDsO","executionInfo":{"status":"ok","timestamp":1725556083854,"user_tz":-330,"elapsed":455,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Execute the compile function\n","compiler.Compiler().compile(\n","    pipeline_func=rlhf_pipeline,\n","    package_path=RLHF_PIPELINE_PKG_PATH\n",")"],"metadata":{"id":"KgpLHJIBKEg3","executionInfo":{"status":"ok","timestamp":1725556096608,"user_tz":-330,"elapsed":453,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Print the first lines of the YAML file\n","!head rlhf_pipeline.yaml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TjLaFvP4KHn7","executionInfo":{"status":"ok","timestamp":1725556107005,"user_tz":-330,"elapsed":455,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}},"outputId":"95f18012-dcbf-491b-c5c6-bcab56646261"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["# PIPELINE DEFINITION\n","# Name: rlhf-train-template\n","# Description: Performs reinforcement learning from human feedback.\n","# Inputs:\n","#    accelerator_type: str [Default: 'GPU']\n","#    deploy_model: bool [Default: True]\n","#    encryption_spec_key_name: str [Default: '']\n","#    eval_dataset: str\n","#    instruction: str\n","#    kl_coeff: float [Default: 0.1]\n"]}]},{"cell_type":"code","source":["!cat rlhf_pipeline.yaml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"t1-RADKxKKKp","executionInfo":{"status":"ok","timestamp":1725556127077,"user_tz":-330,"elapsed":623,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}},"outputId":"70b5d28c-7101-45ee-e812-ccb8bfd12ed9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["# PIPELINE DEFINITION\n","# Name: rlhf-train-template\n","# Description: Performs reinforcement learning from human feedback.\n","# Inputs:\n","#    accelerator_type: str [Default: 'GPU']\n","#    deploy_model: bool [Default: True]\n","#    encryption_spec_key_name: str [Default: '']\n","#    eval_dataset: str\n","#    instruction: str\n","#    kl_coeff: float [Default: 0.1]\n","#    large_model_reference: str\n","#    location: str [Default: '{{$.pipeline_google_cloud_location}}']\n","#    model_display_name: str\n","#    preference_dataset: str\n","#    project: str [Default: '{{$.pipeline_google_cloud_project_id}}']\n","#    prompt_dataset: str\n","#    prompt_sequence_length: int [Default: 512.0]\n","#    reinforcement_learning_rate_multiplier: float [Default: 1.0]\n","#    reinforcement_learning_train_steps: int [Default: 1000.0]\n","#    reward_model_learning_rate_multiplier: float [Default: 1.0]\n","#    reward_model_train_steps: int [Default: 1000.0]\n","#    target_sequence_length: int [Default: 64.0]\n","#    tensorboard_resource_id: str [Default: '']\n","# Outputs:\n","#    endpoint_resource_name: str\n","#    model_resource_name: str\n","components:\n","  comp-bulk-inferrer:\n","    executorLabel: exec-bulk-inferrer\n","    inputDefinitions:\n","      parameters:\n","        accelerator_count:\n","          description: Number of accelerators.\n","          parameterType: NUMBER_INTEGER\n","        accelerator_type:\n","          description: Type of accelerator.\n","          parameterType: STRING\n","        dataset_split:\n","          description: Perform inference on this split of the input dataset.\n","          parameterType: STRING\n","        encryption_spec_key_name:\n","          defaultValue: ''\n","          description: 'Customer-managed encryption key. If this is set,\n","\n","            then all resources created by the CustomJob will be encrypted with the\n","\n","            provided encryption key. Note that this is not supported for TPU at the\n","\n","            moment.'\n","          isOptional: true\n","          parameterType: STRING\n","        image_uri:\n","          parameterType: STRING\n","        input_dataset_path:\n","          description: Path to dataset to use for inference.\n","          parameterType: STRING\n","        input_model:\n","          description: Model to use for inference.\n","          parameterType: STRING\n","        inputs_sequence_length:\n","          description: 'Maximum encoder/prefix length. Inputs will be padded\n","\n","            or truncated to this length.'\n","          parameterType: NUMBER_INTEGER\n","        large_model_reference:\n","          description: Predefined model used to create the ``input_model``.\n","          parameterType: STRING\n","        location:\n","          description: Location used to run the job.\n","          parameterType: STRING\n","        machine_type:\n","          description: Type of machine.\n","          parameterType: STRING\n","        project:\n","          description: Project used to run the job.\n","          parameterType: STRING\n","        sampling_strategy:\n","          defaultValue: greedy\n","          description: The sampling strategy for inference.\n","          isOptional: true\n","          parameterType: STRING\n","        targets_sequence_length:\n","          description: 'Maximum decoder steps. Outputs will be at most this\n","\n","            length.'\n","          parameterType: NUMBER_INTEGER\n","    outputDefinitions:\n","      parameters:\n","        gcp_resources:\n","          description: 'GCP resources that can be used to track the custom finetuning\n","\n","            job.'\n","          parameterType: STRING\n","        output_prediction:\n","          description: Where to save the output prediction.\n","          parameterType: STRING\n","        output_prediction_gcs_path:\n","          parameterType: STRING\n","  comp-condition-1:\n","    dag:\n","      tasks:\n","        condition-2:\n","          componentRef:\n","            name: comp-condition-2\n","          inputs:\n","            parameters:\n","              pipelinechannel--accelerator_type:\n","                componentInputParameter: pipelinechannel--accelerator_type\n","              pipelinechannel--encryption_spec_key_name:\n","                componentInputParameter: pipelinechannel--encryption_spec_key_name\n","              pipelinechannel--eval_dataset:\n","                componentInputParameter: pipelinechannel--eval_dataset\n","              pipelinechannel--instruction:\n","                componentInputParameter: pipelinechannel--instruction\n","              pipelinechannel--large_model_reference:\n","                componentInputParameter: pipelinechannel--large_model_reference\n","              pipelinechannel--location:\n","                componentInputParameter: pipelinechannel--location\n","              pipelinechannel--project:\n","                componentInputParameter: pipelinechannel--project\n","              pipelinechannel--prompt_sequence_length:\n","                componentInputParameter: pipelinechannel--prompt_sequence_length\n","              pipelinechannel--reinforcement-learning-graph-output_model_path:\n","                componentInputParameter: pipelinechannel--reinforcement-learning-graph-output_model_path\n","              pipelinechannel--rlhf-preprocessor-has_inference_dataset:\n","                componentInputParameter: pipelinechannel--rlhf-preprocessor-has_inference_dataset\n","              pipelinechannel--target_sequence_length:\n","                componentInputParameter: pipelinechannel--target_sequence_length\n","          taskInfo:\n","            name: CheckModel Checkpoint Exists\n","          triggerPolicy:\n","            condition: inputs.parameter_values['pipelinechannel--reinforcement-learning-graph-output_model_path']\n","              != ''\n","    inputDefinitions:\n","      parameters:\n","        pipelinechannel--accelerator_type:\n","          parameterType: STRING\n","        pipelinechannel--encryption_spec_key_name:\n","          parameterType: STRING\n","        pipelinechannel--eval_dataset:\n","          parameterType: STRING\n","        pipelinechannel--instruction:\n","          parameterType: STRING\n","        pipelinechannel--large_model_reference:\n","          parameterType: STRING\n","        pipelinechannel--location:\n","          parameterType: STRING\n","        pipelinechannel--project:\n","          parameterType: STRING\n","        pipelinechannel--prompt_sequence_length:\n","          parameterType: NUMBER_INTEGER\n","        pipelinechannel--reinforcement-learning-graph-output_model_path:\n","          parameterType: STRING\n","        pipelinechannel--rlhf-preprocessor-has_inference_dataset:\n","          parameterType: BOOLEAN\n","        pipelinechannel--target_sequence_length:\n","          parameterType: NUMBER_INTEGER\n","  comp-condition-2:\n","    dag:\n","      tasks:\n","        infer-eval-template:\n","          cachingOptions:\n","            enableCache: true\n","          componentRef:\n","            name: comp-infer-eval-template\n","          inputs:\n","            parameters:\n","              accelerator_type:\n","                componentInputParameter: pipelinechannel--accelerator_type\n","              encryption_spec_key_name:\n","                componentInputParameter: pipelinechannel--encryption_spec_key_name\n","              instruction:\n","                componentInputParameter: pipelinechannel--instruction\n","              large_model_reference:\n","                componentInputParameter: pipelinechannel--large_model_reference\n","              location:\n","                componentInputParameter: pipelinechannel--location\n","              model_checkpoint:\n","                componentInputParameter: pipelinechannel--reinforcement-learning-graph-output_model_path\n","              project:\n","                componentInputParameter: pipelinechannel--project\n","              prompt_dataset:\n","                componentInputParameter: pipelinechannel--eval_dataset\n","              prompt_sequence_length:\n","                componentInputParameter: pipelinechannel--prompt_sequence_length\n","              target_sequence_length:\n","                componentInputParameter: pipelinechannel--target_sequence_length\n","          taskInfo:\n","            name: infer-eval-template\n","    inputDefinitions:\n","      parameters:\n","        pipelinechannel--accelerator_type:\n","          parameterType: STRING\n","        pipelinechannel--encryption_spec_key_name:\n","          parameterType: STRING\n","        pipelinechannel--eval_dataset:\n","          parameterType: STRING\n","        pipelinechannel--instruction:\n","          parameterType: STRING\n","        pipelinechannel--large_model_reference:\n","          parameterType: STRING\n","        pipelinechannel--location:\n","          parameterType: STRING\n","        pipelinechannel--project:\n","          parameterType: STRING\n","        pipelinechannel--prompt_sequence_length:\n","          parameterType: NUMBER_INTEGER\n","        pipelinechannel--reinforcement-learning-graph-output_model_path:\n","          parameterType: STRING\n","        pipelinechannel--rlhf-preprocessor-has_inference_dataset:\n","          parameterType: BOOLEAN\n","        pipelinechannel--target_sequence_length:\n","          parameterType: NUMBER_INTEGER\n","  comp-deploy-llm-model:\n","    executorLabel: exec-deploy-llm-model\n","    inputDefinitions:\n","      parameters:\n","        deploy_model:\n","          defaultValue: true\n","          description: 'Whether to deploy the model to an endpoint. Default is\n","\n","            ``True``. If ``False``, the model will not be deployed and output\n","\n","            artifacts will contain empty strings.'\n","          isOptional: true\n","          parameterType: BOOLEAN\n","        display_name:\n","          description: Name of the model (shown in Model Registry).\n","          parameterType: STRING\n","        encryption_spec_key_name:\n","          defaultValue: ''\n","          description: Customer-managed encryption key.\n","          isOptional: true\n","          parameterType: STRING\n","        location:\n","          description: Location for model upload and deployment.\n","          parameterType: STRING\n","        model_resource_name:\n","          description: Path to the created Model on Model Registry.\n","          parameterType: STRING\n","        project:\n","          description: Name of the GCP project.\n","          parameterType: STRING\n","        regional_endpoint:\n","          description: Regional API endpoint.\n","          parameterType: STRING\n","        service_account:\n","          defaultValue: ''\n","          description: If set, then a custom service account will be used.\n","          isOptional: true\n","          parameterType: STRING\n","    outputDefinitions:\n","      parameters:\n","        create_endpoint_gcp_resources:\n","          description: 'Serialized JSON of GCP resources for\n","\n","            creating an endpoint.'\n","          parameterType: STRING\n","        deploy_model_gcp_resources:\n","          description: 'Serialized JSON of GCP resources for deploying\n","\n","            the model.'\n","          parameterType: STRING\n","        endpoint_resource_name:\n","          description: Path to the created endpoint on Online Prediction.\n","          parameterType: STRING\n","  comp-infer-eval-template:\n","    dag:\n","      outputs:\n","        parameters:\n","          output_prediction_gcs_path:\n","            valueFromParameter:\n","              outputParameterKey: output_prediction_gcs_path\n","              producerSubtask: bulk-inferrer\n","      tasks:\n","        bulk-inferrer:\n","          cachingOptions:\n","            enableCache: true\n","          componentRef:\n","            name: comp-bulk-inferrer\n","          dependentTasks:\n","          - infer-preprocessor\n","          - private-text-importer\n","          inputs:\n","            parameters:\n","              accelerator_count:\n","                taskOutputParameter:\n","                  outputParameterKey: metadata_accelerator_count\n","                  producerTask: infer-preprocessor\n","              accelerator_type:\n","                taskOutputParameter:\n","                  outputParameterKey: metadata_accelerator_type\n","                  producerTask: infer-preprocessor\n","              dataset_split:\n","                runtimeValue:\n","                  constant: train\n","              encryption_spec_key_name:\n","                componentInputParameter: encryption_spec_key_name\n","              image_uri:\n","                taskOutputParameter:\n","                  outputParameterKey: metadata_refined_image_uri\n","                  producerTask: infer-preprocessor\n","              input_dataset_path:\n","                taskOutputParameter:\n","                  outputParameterKey: imported_data_path\n","                  producerTask: private-text-importer\n","              input_model:\n","                taskOutputParameter:\n","                  outputParameterKey: metadata_reference_model_path\n","                  producerTask: infer-preprocessor\n","              inputs_sequence_length:\n","                componentInputParameter: prompt_sequence_length\n","              large_model_reference:\n","                taskOutputParameter:\n","                  outputParameterKey: metadata_large_model_reference\n","                  producerTask: infer-preprocessor\n","              location:\n","                taskOutputParameter:\n","                  outputParameterKey: metadata_tuning_location\n","                  producerTask: infer-preprocessor\n","              machine_type:\n","                taskOutputParameter:\n","                  outputParameterKey: metadata_machine_type\n","                  producerTask: infer-preprocessor\n","              project:\n","                componentInputParameter: project\n","              sampling_strategy:\n","                componentInputParameter: sampling_strategy\n","              targets_sequence_length:\n","                componentInputParameter: target_sequence_length\n","          taskInfo:\n","            name: Bulk Inferrer\n","        infer-preprocessor:\n","          cachingOptions:\n","            enableCache: true\n","          componentRef:\n","            name: comp-infer-preprocessor\n","          inputs:\n","            parameters:\n","              accelerator_type:\n","                componentInputParameter: accelerator_type\n","              artifact_registry:\n","                runtimeValue:\n","                  constant: rlhf\n","              instruction:\n","                componentInputParameter: instruction\n","              large_model_reference:\n","                componentInputParameter: large_model_reference\n","              location:\n","                runtimeValue:\n","                  constant: us\n","              project:\n","                runtimeValue:\n","                  constant: vertex-ai-restricted\n","              tag:\n","                runtimeValue:\n","                  constant: '20240623_1707'\n","              use_test_spec:\n","                runtimeValue:\n","                  constant: false\n","          taskInfo:\n","            name: Preprocess Inputs\n","        preprocess-chat-dataset:\n","          cachingOptions:\n","            enableCache: true\n","          componentRef:\n","            name: comp-preprocess-chat-dataset-3\n","          inputs:\n","            parameters:\n","              dataset_type:\n","                runtimeValue:\n","                  constant: prompt\n","              default_context:\n","                componentInputParameter: instruction\n","              input_dataset_uri:\n","                componentInputParameter: prompt_dataset\n","              large_model_reference:\n","                componentInputParameter: large_model_reference\n","          taskInfo:\n","            name: Preprocess Dataset\n","        private-text-importer:\n","          cachingOptions: {}\n","          componentRef:\n","            name: comp-private-text-importer-2\n","          dependentTasks:\n","          - infer-preprocessor\n","          - preprocess-chat-dataset\n","          inputs:\n","            parameters:\n","              encryption_spec_key_name:\n","                componentInputParameter: encryption_spec_key_name\n","              input_text:\n","                taskOutputParameter:\n","                  outputParameterKey: processed_dataset_uri\n","                  producerTask: preprocess-chat-dataset\n","              inputs_field_name:\n","                runtimeValue:\n","                  constant: input_text\n","              instruction:\n","                taskOutputParameter:\n","                  outputParameterKey: metadata_instruction\n","                  producerTask: infer-preprocessor\n","              large_model_reference:\n","                taskOutputParameter:\n","                  outputParameterKey: metadata_large_model_reference\n","                  producerTask: infer-preprocessor\n","              location:\n","                componentInputParameter: location\n","              output_split_name:\n","                runtimeValue:\n","                  constant: train\n","              project:\n","                componentInputParameter: project\n","              targets_field_name:\n","                runtimeValue:\n","                  constant: ''\n","          taskInfo:\n","            name: Import Prompt Dataset\n","    inputDefinitions:\n","      parameters:\n","        accelerator_type:\n","          defaultValue: GPU\n","          description: One of 'TPU' or 'GPU'. If 'TPU' is specified, tuning components\n","            run in europe-west4. Otherwise tuning components run in us-central1 on\n","            GPUs. Default is 'GPU'.\n","          isOptional: true\n","          parameterType: STRING\n","        encryption_spec_key_name:\n","          defaultValue: ''\n","          description: Customer-managed encryption key. If this is set, then all resources\n","            created by the CustomJob will be encrypted with the provided encryption\n","            key. Note that this is not supported for TPU at the moment.\n","          isOptional: true\n","          parameterType: STRING\n","        instruction:\n","          description: This field lets the model know what task it needs to perform.\n","            Base models have been trained over a large set of varied instructions.\n","            You can give a simple and intuitive description of the task and the model\n","            will follow it, e.g. \"Classify this movie review as positive or negative\"\n","            or \"Translate this sentence to Danish\". Do not specify this if your dataset\n","            already prepends the instruction to the inputs field.\n","          isOptional: true\n","          parameterType: STRING\n","        large_model_reference:\n","          description: Name of the base model. Supported values are `text-bison@001`,\n","            `t5-small`, `t5-large`, `t5-xl` and `t5-xxl`. `text-bison@001` and `t5-small`\n","            are supported in `us-central1` and `europe-west4`. `t5-large`, `t5-xl`\n","            and `t5-xxl` are only supported in `europe-west4`.\n","          parameterType: STRING\n","        location:\n","          defaultValue: '{{$.pipeline_google_cloud_location}}'\n","          description: Location used to run non-tuning components, i.e. components\n","            that do not require accelerators. If not specified the location used to\n","            run the pipeline will be used.\n","          isOptional: true\n","          parameterType: STRING\n","        model_checkpoint:\n","          description: Optional Cloud storage path to the model checkpoint. If not\n","            provided, the default checkpoint for the `large_model_reference` will\n","            be used.\n","          isOptional: true\n","          parameterType: STRING\n","        project:\n","          defaultValue: '{{$.pipeline_google_cloud_project_id}}'\n","          description: Project used to run custom jobs. If not specified the project\n","            used to run the pipeline will be used.\n","          isOptional: true\n","          parameterType: STRING\n","        prompt_dataset:\n","          description: Cloud storage path to an unlabled JSONL dataset that contains\n","            prompts. Text datasets must contain an `input_text` field that contains\n","            the prompt. Chat datasets must contain at least 1 message in a `messages`\n","            field. Each message must be valid JSON that contains `author` and `content`\n","            fields, where valid `author` values are `user` and `assistant` and `content`\n","            must be non-empty. Each row may contain multiple messages, but the first\n","            and last author must be the `user`. An optional `context` field may be\n","            provided for each example in a chat dataset. If provided, the `context`\n","            will preprended to the message `content`. The `instruction` serves as\n","            the default context. (Useful if most messages use the same system-level\n","            context.) Any context provided in the example will override the default\n","            value.\n","          parameterType: STRING\n","        prompt_sequence_length:\n","          defaultValue: 512.0\n","          description: Maximum tokenized sequence length for input text. Higher values\n","            increase memory overhead. This value should be at most 8192. Default value\n","            is 512.\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        sampling_strategy:\n","          defaultValue: greedy\n","          description: This field specifies the sampling strategy. The valid options\n","            are 'greedy' and 'temperature_sampling'.\n","          isOptional: true\n","          parameterType: STRING\n","        target_sequence_length:\n","          defaultValue: 64.0\n","          description: ' Maximum tokenized sequence length for target text. Higher\n","            values increase memory overhead. This value should be at most 1024. Default\n","            value is 64.'\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","    outputDefinitions:\n","      parameters:\n","        output_prediction_gcs_path:\n","          parameterType: STRING\n","  comp-infer-preprocessor:\n","    executorLabel: exec-infer-preprocessor\n","    inputDefinitions:\n","      parameters:\n","        accelerator_type:\n","          description: Specific accelerator type for the job.\n","          parameterType: STRING\n","        artifact_registry:\n","          description: Registry that contains Docker images.\n","          parameterType: STRING\n","        image_uri:\n","          defaultValue: us-docker.pkg.dev/vertex-ai-restricted/rlhf/refined_cpu:20240623_1707\n","          description: Docker image URI to use for the custom job.\n","          isOptional: true\n","          parameterType: STRING\n","        input_reference_model_path:\n","          defaultValue: ''\n","          description: The model checkpoint path for the reference model\n","          isOptional: true\n","          parameterType: STRING\n","        instruction:\n","          defaultValue: ''\n","          description: The instruction to let the model know what task it needs to\n","            perform.\n","          isOptional: true\n","          parameterType: STRING\n","        large_model_reference:\n","          description: The model for fine tuning.\n","          parameterType: STRING\n","        location:\n","          description: Region that contains the artifact registry.\n","          parameterType: STRING\n","        project:\n","          description: Project that contains the artifact registry.\n","          parameterType: STRING\n","        tag:\n","          description: Image tag.\n","          parameterType: STRING\n","        use_experimental_image:\n","          defaultValue: false\n","          description: ' Whether to use refined experimental image.'\n","          isOptional: true\n","          parameterType: BOOLEAN\n","        use_test_spec:\n","          description: Whether to use a lower resource machine for testing.\n","          parameterType: BOOLEAN\n","    outputDefinitions:\n","      parameters:\n","        gcp_resources:\n","          description: GCP resources that can be used to track the custom job.\n","          parameterType: STRING\n","        metadata_accelerator_count:\n","          description: The number of accelerator.\n","          parameterType: NUMBER_INTEGER\n","        metadata_accelerator_type:\n","          description: Specific accelerator type for the custom job.\n","          parameterType: STRING\n","        metadata_instruction:\n","          description: The instruction to let the model know what task it needs to\n","            perform.\n","          parameterType: STRING\n","        metadata_large_model_reference:\n","          description: The base model for fine tuning. The name should be in capitalized\n","            snake case format.\n","          parameterType: STRING\n","        metadata_machine_type:\n","          description: The type of the machine to provision for the custom job.\n","          parameterType: STRING\n","        metadata_reference_model_path:\n","          description: The model checkpoint path for the reinforcer model\n","          parameterType: STRING\n","        metadata_refined_image_uri:\n","          description: Docker image URI to use for the custom job.\n","          parameterType: STRING\n","        metadata_reward_model_path:\n","          description: The model checkpoint path for the reward model.\n","          parameterType: STRING\n","        metadata_reward_model_reference:\n","          description: ' The base model for training reward model. The name should\n","            be in capitalized snake case format.'\n","          parameterType: STRING\n","        metadata_tuning_location:\n","          description: The GCP region to run the custom job.\n","          parameterType: STRING\n","  comp-llm-deployment-graph:\n","    dag:\n","      outputs:\n","        parameters:\n","          endpoint_resource_name:\n","            valueFromParameter:\n","              outputParameterKey: endpoint_resource_name\n","              producerSubtask: deploy-llm-model\n","          model_resource_name:\n","            valueFromParameter:\n","              outputParameterKey: model_resource_name\n","              producerSubtask: refined-upload-llm-model\n","      tasks:\n","        deploy-llm-model:\n","          cachingOptions:\n","            enableCache: true\n","          componentRef:\n","            name: comp-deploy-llm-model\n","          dependentTasks:\n","          - refined-upload-llm-model\n","          inputs:\n","            parameters:\n","              deploy_model:\n","                componentInputParameter: deploy_model\n","              display_name:\n","                componentInputParameter: model_display_name\n","              encryption_spec_key_name:\n","                componentInputParameter: encryption_spec_key_name\n","              location:\n","                componentInputParameter: upload_location\n","              model_resource_name:\n","                taskOutputParameter:\n","                  outputParameterKey: model_resource_name\n","                  producerTask: refined-upload-llm-model\n","              project:\n","                runtimeValue:\n","                  constant: '{{$.pipeline_google_cloud_project_id}}'\n","              regional_endpoint:\n","                componentInputParameter: regional_endpoint\n","          taskInfo:\n","            name: Deploy Model\n","        refined-upload-llm-model:\n","          cachingOptions:\n","            enableCache: true\n","          componentRef:\n","            name: comp-refined-upload-llm-model\n","          inputs:\n","            parameters:\n","              artifact_uri:\n","                componentInputParameter: output_adapter_path\n","              encryption_spec_key_name:\n","                componentInputParameter: encryption_spec_key_name\n","              location:\n","                componentInputParameter: upload_location\n","              model_display_name:\n","                componentInputParameter: model_display_name\n","              model_reference_name:\n","                componentInputParameter: large_model_reference\n","              project:\n","                runtimeValue:\n","                  constant: '{{$.pipeline_google_cloud_project_id}}'\n","              regional_endpoint:\n","                componentInputParameter: regional_endpoint\n","              tune_type:\n","                runtimeValue:\n","                  constant: rlhf\n","              upload_model:\n","                componentInputParameter: upload_model\n","          taskInfo:\n","            name: Upload Model\n","    inputDefinitions:\n","      parameters:\n","        deploy_model:\n","          defaultValue: true\n","          description: Whether to deploy the model to an endpoint in `us-central1`.\n","            Default is True.\n","          isOptional: true\n","          parameterType: BOOLEAN\n","        encryption_spec_key_name:\n","          defaultValue: ''\n","          description: Customer-managed encryption key. If this is set, then all resources\n","            created by the CustomJob will be encrypted with the provided encryption\n","            key. Note that this is not supported for TPU at the moment.\n","          isOptional: true\n","          parameterType: STRING\n","        large_model_reference:\n","          description: Name of the base model. Supported values are `text-bison@001`,\n","            `t5-small`, `t5-large`, `t5-xl` and `t5-xxl`. `text-bison@001` and `t5-small`\n","            are supported in `us-central1` and `europe-west4`. `t5-large`, `t5-xl`\n","            and `t5-xxl` are only supported in `europe-west4`.\n","          parameterType: STRING\n","        model_display_name:\n","          description: Name of the fine-tuned model shown in the Model Registry. If\n","            not provided, a default name will be created.\n","          isOptional: true\n","          parameterType: STRING\n","        output_adapter_path:\n","          description: Path to the trained model adapter if LoRA tuning was used.\n","          parameterType: STRING\n","        policy_model_reference:\n","          description: The name of the model for deployment. The name should be in\n","            capitalized snake case format.\n","          parameterType: STRING\n","        regional_endpoint:\n","          defaultValue: ''\n","          description: Regional endpoint to upload the model.\n","          isOptional: true\n","          parameterType: STRING\n","        upload_location:\n","          defaultValue: '{{$.pipeline_google_cloud_location}}'\n","          description: Region to upload and deploy the model to. Default is the location\n","            used to run the pipeline components.\n","          isOptional: true\n","          parameterType: STRING\n","        upload_model:\n","          defaultValue: true\n","          isOptional: true\n","          parameterType: BOOLEAN\n","    outputDefinitions:\n","      parameters:\n","        endpoint_resource_name:\n","          description: Path the Online Prediction Endpoint. This will be an empty\n","            string if the model was not deployed.\n","          parameterType: STRING\n","        model_resource_name:\n","          description: Path to the model uploaded to the Model Registry. This will\n","            be an empty string if the model was not deployed.\n","          parameterType: STRING\n","  comp-preprocess-chat-dataset:\n","    executorLabel: exec-preprocess-chat-dataset\n","    inputDefinitions:\n","      parameters:\n","        allow_local_files:\n","          defaultValue: false\n","          description: Whether input URIs can specify local file paths.\n","          isOptional: true\n","          parameterType: BOOLEAN\n","        dataset_type:\n","          parameterType: STRING\n","        default_context:\n","          defaultValue: ''\n","          description: Default context to apply to each example if a chat model is\n","            specified.\n","          isOptional: true\n","          parameterType: STRING\n","        input_dataset_uri:\n","          description: Path to an unprocessed JSONL dataset.\n","          parameterType: STRING\n","        large_model_reference:\n","          description: Name of the base model. Supported values are `text-bison@001`,\n","            `chat-bison@001`, `t5-small`, `t5-large`, `t5-xl` and `t5-xxl`. `text-bison@001`,\n","            `chat-bison@001` and `t5-small` are supported in ``us-central1` and `europe-west4`.\n","            `t5-large`, `t5-xl` and `t5-xxl` are only supported in `europe-west4`.\n","          parameterType: STRING\n","    outputDefinitions:\n","      artifacts:\n","        processed_dataset:\n","          artifactType:\n","            schemaTitle: system.Artifact\n","            schemaVersion: 0.0.1\n","          description: Processed chat dataset. Each example will contain fields `input_text`,\n","            and if the input dataset is not a prompt dataset example will also contain\n","            `output_text`.\n","      parameters:\n","        processed_dataset_uri:\n","          description: String pattern that can be used to find the processed dataset\n","            in downstream components.\n","          parameterType: STRING\n","  comp-preprocess-chat-dataset-2:\n","    executorLabel: exec-preprocess-chat-dataset-2\n","    inputDefinitions:\n","      parameters:\n","        allow_local_files:\n","          defaultValue: false\n","          description: Whether input URIs can specify local file paths.\n","          isOptional: true\n","          parameterType: BOOLEAN\n","        dataset_type:\n","          parameterType: STRING\n","        default_context:\n","          defaultValue: ''\n","          description: Default context to apply to each example if a chat model is\n","            specified.\n","          isOptional: true\n","          parameterType: STRING\n","        input_dataset_uri:\n","          description: Path to an unprocessed JSONL dataset.\n","          parameterType: STRING\n","        large_model_reference:\n","          description: Name of the base model. Supported values are `text-bison@001`,\n","            `chat-bison@001`, `t5-small`, `t5-large`, `t5-xl` and `t5-xxl`. `text-bison@001`,\n","            `chat-bison@001` and `t5-small` are supported in ``us-central1` and `europe-west4`.\n","            `t5-large`, `t5-xl` and `t5-xxl` are only supported in `europe-west4`.\n","          parameterType: STRING\n","    outputDefinitions:\n","      artifacts:\n","        processed_dataset:\n","          artifactType:\n","            schemaTitle: system.Artifact\n","            schemaVersion: 0.0.1\n","          description: Processed chat dataset. Each example will contain fields `input_text`,\n","            and if the input dataset is not a prompt dataset example will also contain\n","            `output_text`.\n","      parameters:\n","        processed_dataset_uri:\n","          description: String pattern that can be used to find the processed dataset\n","            in downstream components.\n","          parameterType: STRING\n","  comp-preprocess-chat-dataset-3:\n","    executorLabel: exec-preprocess-chat-dataset-3\n","    inputDefinitions:\n","      parameters:\n","        allow_local_files:\n","          defaultValue: false\n","          description: Whether input URIs can specify local file paths.\n","          isOptional: true\n","          parameterType: BOOLEAN\n","        dataset_type:\n","          parameterType: STRING\n","        default_context:\n","          defaultValue: ''\n","          description: Default context to apply to each example if a chat model is\n","            specified.\n","          isOptional: true\n","          parameterType: STRING\n","        input_dataset_uri:\n","          description: Path to an unprocessed JSONL dataset.\n","          parameterType: STRING\n","        large_model_reference:\n","          description: Name of the base model. Supported values are `text-bison@001`,\n","            `chat-bison@001`, `t5-small`, `t5-large`, `t5-xl` and `t5-xxl`. `text-bison@001`,\n","            `chat-bison@001` and `t5-small` are supported in ``us-central1` and `europe-west4`.\n","            `t5-large`, `t5-xl` and `t5-xxl` are only supported in `europe-west4`.\n","          parameterType: STRING\n","    outputDefinitions:\n","      artifacts:\n","        processed_dataset:\n","          artifactType:\n","            schemaTitle: system.Artifact\n","            schemaVersion: 0.0.1\n","          description: Processed chat dataset. Each example will contain fields `input_text`,\n","            and if the input dataset is not a prompt dataset example will also contain\n","            `output_text`.\n","      parameters:\n","        processed_dataset_uri:\n","          description: String pattern that can be used to find the processed dataset\n","            in downstream components.\n","          parameterType: STRING\n","  comp-private-text-comparison-importer:\n","    executorLabel: exec-private-text-comparison-importer\n","    inputDefinitions:\n","      parameters:\n","        choice_field_name:\n","          description: 'Name of field that specifies the index of the best\n","\n","            candidate.'\n","          parameterType: STRING\n","        comma_separated_candidates_field_names:\n","          description: 'Comma separated list of fields that\n","\n","            contain candidate text, e.g. ``''field_1,field_2,field_3''``.'\n","          parameterType: STRING\n","        encryption_spec_key_name:\n","          defaultValue: ''\n","          description: 'Customer-managed encryption key. If this is set,\n","\n","            then all resources created by the CustomJob will be encrypted with the\n","\n","            provided encryption key. Note that this is not supported for TPU at the\n","\n","            moment.'\n","          isOptional: true\n","          parameterType: STRING\n","        image_uri:\n","          defaultValue: us-docker.pkg.dev/vertex-ai-restricted/rlhf/refined_cpu:20240623_1707\n","          description: Optional location of the text comparison importer image.\n","          isOptional: true\n","          parameterType: STRING\n","        input_text:\n","          description: Path to text data. Supports glob patterns.\n","          parameterType: STRING\n","        inputs_field_name:\n","          description: Name of field that contains input text.\n","          parameterType: STRING\n","        instruction:\n","          defaultValue: ''\n","          description: Optional instruction to prepend to inputs field.\n","          isOptional: true\n","          parameterType: STRING\n","        large_model_reference:\n","          description: 'Predefined model used to create the model to be\n","\n","            trained. This paramerter is used for obtaining model vocabulary because\n","\n","            this component tokenizes and then caches the tokenized tasks.'\n","          parameterType: STRING\n","        location:\n","          description: Location used to run the job.\n","          parameterType: STRING\n","        machine_type:\n","          defaultValue: e2-highmem-8\n","          description: The type of the machine to provision for the custom job.\n","          isOptional: true\n","          parameterType: STRING\n","        project:\n","          description: Project used to run the job.\n","          parameterType: STRING\n","        split:\n","          description: 'The created seqio task has 1 split, its name is specified\n","            by this\n","\n","            argument.'\n","          parameterType: STRING\n","    outputDefinitions:\n","      parameters:\n","        gcp_resources:\n","          description: GCP resources that can be used to track the custom job.\n","          parameterType: STRING\n","        output_dataset_path:\n","          description: Path to cached SeqIO task created from input dataset.\n","          parameterType: STRING\n","  comp-private-text-comparison-importer-2:\n","    executorLabel: exec-private-text-comparison-importer-2\n","    inputDefinitions:\n","      parameters:\n","        choice_field_name:\n","          description: 'Name of field that specifies the index of the best\n","\n","            candidate.'\n","          parameterType: STRING\n","        comma_separated_candidates_field_names:\n","          description: 'Comma separated list of fields that\n","\n","            contain candidate text, e.g. ``''field_1,field_2,field_3''``.'\n","          parameterType: STRING\n","        encryption_spec_key_name:\n","          defaultValue: ''\n","          description: 'Customer-managed encryption key. If this is set,\n","\n","            then all resources created by the CustomJob will be encrypted with the\n","\n","            provided encryption key. Note that this is not supported for TPU at the\n","\n","            moment.'\n","          isOptional: true\n","          parameterType: STRING\n","        image_uri:\n","          defaultValue: us-docker.pkg.dev/vertex-ai-restricted/rlhf/refined_cpu:20240623_1707\n","          description: Optional location of the text comparison importer image.\n","          isOptional: true\n","          parameterType: STRING\n","        input_text:\n","          description: Path to text data. Supports glob patterns.\n","          parameterType: STRING\n","        inputs_field_name:\n","          description: Name of field that contains input text.\n","          parameterType: STRING\n","        instruction:\n","          defaultValue: ''\n","          description: Optional instruction to prepend to inputs field.\n","          isOptional: true\n","          parameterType: STRING\n","        large_model_reference:\n","          description: 'Predefined model used to create the model to be\n","\n","            trained. This paramerter is used for obtaining model vocabulary because\n","\n","            this component tokenizes and then caches the tokenized tasks.'\n","          parameterType: STRING\n","        location:\n","          description: Location used to run the job.\n","          parameterType: STRING\n","        machine_type:\n","          defaultValue: e2-highmem-8\n","          description: The type of the machine to provision for the custom job.\n","          isOptional: true\n","          parameterType: STRING\n","        project:\n","          description: Project used to run the job.\n","          parameterType: STRING\n","        split:\n","          description: 'The created seqio task has 1 split, its name is specified\n","            by this\n","\n","            argument.'\n","          parameterType: STRING\n","    outputDefinitions:\n","      parameters:\n","        gcp_resources:\n","          description: GCP resources that can be used to track the custom job.\n","          parameterType: STRING\n","        output_dataset_path:\n","          description: Path to cached SeqIO task created from input dataset.\n","          parameterType: STRING\n","  comp-private-text-importer:\n","    executorLabel: exec-private-text-importer\n","    inputDefinitions:\n","      parameters:\n","        encryption_spec_key_name:\n","          defaultValue: ''\n","          description: 'Customer-managed encryption key. If this is set,\n","\n","            then all resources created by the CustomJob will be encrypted with the\n","\n","            provided encryption key. Note that this is not supported for TPU at the\n","\n","            moment.'\n","          isOptional: true\n","          parameterType: STRING\n","        image_uri:\n","          defaultValue: us-docker.pkg.dev/vertex-ai-restricted/rlhf/refined_cpu:20240623_1707\n","          description: Optional location of the text importer image.\n","          isOptional: true\n","          parameterType: STRING\n","        input_text:\n","          description: Path to text data. Supports glob patterns.\n","          parameterType: STRING\n","        inputs_field_name:\n","          description: Name of field that contains input text.\n","          parameterType: STRING\n","        instruction:\n","          defaultValue: ''\n","          description: Optional instruction to prepend to inputs field.\n","          isOptional: true\n","          parameterType: STRING\n","        large_model_reference:\n","          description: 'Predefined model used to create the model to be\n","\n","            trained. This paramerter is used for obtaining model vocabulary because\n","\n","            this component tokenizes and then caches the tokenized tasks.'\n","          parameterType: STRING\n","        location:\n","          description: Location used to run the job.\n","          parameterType: STRING\n","        machine_type:\n","          defaultValue: e2-highmem-8\n","          description: The type of the machine to provision for the custom job.\n","          isOptional: true\n","          parameterType: STRING\n","        max_num_input_examples:\n","          description: Maximum number of examples to import.\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        output_split_name:\n","          defaultValue: all\n","          description: 'The created seqio task has 1 split, its name is specified\n","\n","            by this argument.'\n","          isOptional: true\n","          parameterType: STRING\n","        project:\n","          description: Project used to run the job.\n","          parameterType: STRING\n","        targets_field_name:\n","          description: Name of field that contains target text.\n","          parameterType: STRING\n","    outputDefinitions:\n","      artifacts:\n","        imported_data:\n","          artifactType:\n","            schemaTitle: system.Dataset\n","            schemaVersion: 0.0.1\n","          description: Artifact representing the imported data and cached Tasks.\n","      parameters:\n","        gcp_resources:\n","          description: Tracker for GCP resources created by this component.\n","          parameterType: STRING\n","        imported_data_path:\n","          description: Path to cached SeqIO task created from input dataset.\n","          parameterType: STRING\n","  comp-private-text-importer-2:\n","    executorLabel: exec-private-text-importer-2\n","    inputDefinitions:\n","      parameters:\n","        encryption_spec_key_name:\n","          defaultValue: ''\n","          description: 'Customer-managed encryption key. If this is set,\n","\n","            then all resources created by the CustomJob will be encrypted with the\n","\n","            provided encryption key. Note that this is not supported for TPU at the\n","\n","            moment.'\n","          isOptional: true\n","          parameterType: STRING\n","        image_uri:\n","          defaultValue: us-docker.pkg.dev/vertex-ai-restricted/rlhf/refined_cpu:20240623_1707\n","          description: Optional location of the text importer image.\n","          isOptional: true\n","          parameterType: STRING\n","        input_text:\n","          description: Path to text data. Supports glob patterns.\n","          parameterType: STRING\n","        inputs_field_name:\n","          description: Name of field that contains input text.\n","          parameterType: STRING\n","        instruction:\n","          defaultValue: ''\n","          description: Optional instruction to prepend to inputs field.\n","          isOptional: true\n","          parameterType: STRING\n","        large_model_reference:\n","          description: 'Predefined model used to create the model to be\n","\n","            trained. This paramerter is used for obtaining model vocabulary because\n","\n","            this component tokenizes and then caches the tokenized tasks.'\n","          parameterType: STRING\n","        location:\n","          description: Location used to run the job.\n","          parameterType: STRING\n","        machine_type:\n","          defaultValue: e2-highmem-8\n","          description: The type of the machine to provision for the custom job.\n","          isOptional: true\n","          parameterType: STRING\n","        max_num_input_examples:\n","          description: Maximum number of examples to import.\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        output_split_name:\n","          defaultValue: all\n","          description: 'The created seqio task has 1 split, its name is specified\n","\n","            by this argument.'\n","          isOptional: true\n","          parameterType: STRING\n","        project:\n","          description: Project used to run the job.\n","          parameterType: STRING\n","        targets_field_name:\n","          description: Name of field that contains target text.\n","          parameterType: STRING\n","    outputDefinitions:\n","      artifacts:\n","        imported_data:\n","          artifactType:\n","            schemaTitle: system.Dataset\n","            schemaVersion: 0.0.1\n","          description: Artifact representing the imported data and cached Tasks.\n","      parameters:\n","        gcp_resources:\n","          description: Tracker for GCP resources created by this component.\n","          parameterType: STRING\n","        imported_data_path:\n","          description: Path to cached SeqIO task created from input dataset.\n","          parameterType: STRING\n","  comp-refined-upload-llm-model:\n","    executorLabel: exec-refined-upload-llm-model\n","    inputDefinitions:\n","      parameters:\n","        artifact_uri:\n","          description: Path to the artifact to upload.\n","          parameterType: STRING\n","        encryption_spec_key_name:\n","          defaultValue: ''\n","          description: Customer-managed encryption key.\n","          isOptional: true\n","          parameterType: STRING\n","        location:\n","          description: Location for model upload and deployment.\n","          parameterType: STRING\n","        model_display_name:\n","          description: Name of the model (shown in Model Registry).\n","          parameterType: STRING\n","        model_reference_name:\n","          description: Large model reference name.\n","          parameterType: STRING\n","        project:\n","          description: Name of the GCP project.\n","          parameterType: STRING\n","        regional_endpoint:\n","          description: Regional API endpoint.\n","          parameterType: STRING\n","        tune_type:\n","          defaultValue: ''\n","          description: 'Method used to tune the model, e.g. ``rlhf``. If present,\n","            this\n","\n","            value is used to set the ``tune-type`` run label during model upload.'\n","          isOptional: true\n","          parameterType: STRING\n","        upload_model:\n","          defaultValue: true\n","          description: 'Whether to upload the model to the Model Registry. Default\n","\n","            is ``True``. If ``False``, the model will not be uploaded and output\n","\n","            artifacts will contain empty strings.'\n","          isOptional: true\n","          parameterType: BOOLEAN\n","    outputDefinitions:\n","      parameters:\n","        gcp_resources:\n","          description: Serialized JSON of `gcp_resources`.\n","          parameterType: STRING\n","        model_resource_name:\n","          description: Path to the created Model on Model Registry.\n","          parameterType: STRING\n","  comp-reinforcement-learning-graph:\n","    dag:\n","      outputs:\n","        parameters:\n","          output_adapter_path:\n","            valueFromParameter:\n","              outputParameterKey: output_adapter_path\n","              producerSubtask: reinforcer\n","          output_model_path:\n","            valueFromParameter:\n","              outputParameterKey: output_model_path\n","              producerSubtask: reinforcer\n","      tasks:\n","        preprocess-chat-dataset:\n","          cachingOptions:\n","            enableCache: true\n","          componentRef:\n","            name: comp-preprocess-chat-dataset-2\n","          inputs:\n","            parameters:\n","              dataset_type:\n","                runtimeValue:\n","                  constant: prompt\n","              default_context:\n","                componentInputParameter: instruction\n","              input_dataset_uri:\n","                componentInputParameter: prompt_dataset\n","              large_model_reference:\n","                componentInputParameter: large_model_reference\n","          taskInfo:\n","            name: Preprocess Prompt Dataset\n","        private-text-importer:\n","          cachingOptions: {}\n","          componentRef:\n","            name: comp-private-text-importer\n","          dependentTasks:\n","          - preprocess-chat-dataset\n","          inputs:\n","            parameters:\n","              encryption_spec_key_name:\n","                componentInputParameter: encryption_spec_key_name\n","              input_text:\n","                taskOutputParameter:\n","                  outputParameterKey: processed_dataset_uri\n","                  producerTask: preprocess-chat-dataset\n","              inputs_field_name:\n","                runtimeValue:\n","                  constant: input_text\n","              instruction:\n","                componentInputParameter: instruction\n","              large_model_reference:\n","                componentInputParameter: policy_model_reference\n","              location:\n","                componentInputParameter: location\n","              output_split_name:\n","                runtimeValue:\n","                  constant: train\n","              project:\n","                componentInputParameter: project\n","              targets_field_name:\n","                runtimeValue:\n","                  constant: non_existent_targets_field_name\n","          taskInfo:\n","            name: Import Prompt Dataset\n","        reinforcer:\n","          cachingOptions: {}\n","          componentRef:\n","            name: comp-reinforcer\n","          dependentTasks:\n","          - private-text-importer\n","          inputs:\n","            parameters:\n","              accelerator_count:\n","                componentInputParameter: accelerator_count\n","              accelerator_type:\n","                componentInputParameter: accelerator_type\n","              batch_size:\n","                componentInputParameter: batch_size\n","              encryption_spec_key_name:\n","                componentInputParameter: encryption_spec_key_name\n","              image_uri:\n","                componentInputParameter: rl_image_uri\n","              input_dataset_path:\n","                taskOutputParameter:\n","                  outputParameterKey: imported_data_path\n","                  producerTask: private-text-importer\n","              input_preference_dataset_path:\n","                componentInputParameter: input_preference_dataset_path\n","              input_reference_model_path:\n","                componentInputParameter: policy_model_path\n","              input_reward_adapter_path:\n","                componentInputParameter: input_reward_adapter_path\n","              input_reward_model_path:\n","                componentInputParameter: input_reward_model_path\n","              inputs_sequence_length:\n","                componentInputParameter: prompt_sequence_length\n","              kl_coeff:\n","                componentInputParameter: kl_coeff\n","              large_model_reference:\n","                componentInputParameter: policy_model_reference\n","              learning_rate_multiplier:\n","                componentInputParameter: reinforcement_learning_rate_multiplier\n","              location:\n","                componentInputParameter: tuning_location\n","              lora_dim:\n","                componentInputParameter: lora_dim\n","              machine_type:\n","                componentInputParameter: machine_type\n","              num_microbatches:\n","                componentInputParameter: num_microbatches\n","              project:\n","                componentInputParameter: project\n","              reward_lora_dim:\n","                componentInputParameter: reward_lora_dim\n","              reward_model_reference:\n","                componentInputParameter: reward_model_reference\n","              targets_sequence_length:\n","                componentInputParameter: target_sequence_length\n","              tensorboard_resource_id:\n","                componentInputParameter: tensorboard_resource_id\n","              train_steps:\n","                componentInputParameter: reinforcement_learning_train_steps\n","          taskInfo:\n","            name: Reinforcer\n","    inputDefinitions:\n","      parameters:\n","        accelerator_count:\n","          description: The number of accelerator.\n","          parameterType: NUMBER_INTEGER\n","        accelerator_type:\n","          description: Specific accelerator type for the custom job.\n","          parameterType: STRING\n","        batch_size:\n","          defaultValue: 64.0\n","          description: Number of examples in each finetuning step. Default is 64.\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        encryption_spec_key_name:\n","          defaultValue: ''\n","          description: Customer-managed encryption key. If this is set, then all resources\n","            created by the CustomJob will be encrypted with the provided encryption\n","            key. Note that this is not supported for TPU at the moment.\n","          isOptional: true\n","          parameterType: STRING\n","        input_preference_dataset_path:\n","          description: Path to preference dataset used by the reward model.\n","          parameterType: STRING\n","        input_reward_adapter_path:\n","          description: Path to the reward LoRA adapter to use during reinforcement\n","            learning.\n","          parameterType: STRING\n","        input_reward_model_path:\n","          parameterType: STRING\n","        instruction:\n","          description: This field lets the model know what task it needs to perform.\n","            Base models have been trained over a large set of varied instructions.\n","            You can give a simple and intuitive description of the task and the model\n","            will follow it, e.g. \"Classify this movie review as positive or negative\"\n","            or \"Translate this sentence to Danish\". Do not specify this if your dataset\n","            already prepends the instruction to the inputs field.\n","          isOptional: true\n","          parameterType: STRING\n","        kl_coeff:\n","          defaultValue: 0.1\n","          description: Coefficient for KL penalty. This regularizes the policy model\n","            and penalizes if it diverges from its initial distribution. If set to\n","            0, the reference language model is not loaded into memory. Default value\n","            is 0.1.\n","          isOptional: true\n","          parameterType: NUMBER_DOUBLE\n","        large_model_reference:\n","          description: Name of the base model. Supported values are `text-bison@001`,\n","            `t5-small`, `t5-large`, `t5-xl` and `t5-xxl`. `text-bison@001` and `t5-small`\n","            are supported in `us-central1` and `europe-west4`. `t5-large`, `t5-xl`\n","            and `t5-xxl` are only supported in `europe-west4`.\n","          parameterType: STRING\n","        location:\n","          defaultValue: '{{$.pipeline_google_cloud_location}}'\n","          description: Location used to run non-tuning components, i.e. components\n","            that do not require accelerators. If not specified the location used to\n","            run the pipeline will be used.\n","          isOptional: true\n","          parameterType: STRING\n","        lora_dim:\n","          defaultValue: 1.0\n","          description: The rank of the LoRA adapter. If >0, then use LoRA-tuning.\n","            If =0, then use full-tuning. Default is 1.\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        machine_type:\n","          description: The type of the machine to provision for the custom job. Must\n","            be a valid GCE instance type and compatible with the accelerator type.\n","          parameterType: STRING\n","        num_microbatches:\n","          defaultValue: 0.0\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        policy_model_path:\n","          description: The model checkpoint path to the reinforcer model.\n","          parameterType: STRING\n","        policy_model_reference:\n","          description: Name of the policy model. The name should be in capitalized\n","            snake case format.\n","          parameterType: STRING\n","        project:\n","          defaultValue: '{{$.pipeline_google_cloud_project_id}}'\n","          description: Project used to run custom jobs. If not specified the project\n","            used to run the pipeline will be used.\n","          isOptional: true\n","          parameterType: STRING\n","        prompt_dataset:\n","          description: Cloud storage path to an unlabled JSONL dataset that contains\n","            prompts. Text datasets must contain an `input_text` field that contains\n","            the prompt. Chat datasets must contain at least 1 message in a `messages`\n","            field. Each message must be valid JSON that contains `author` and `content`\n","            fields, where valid `author` values are `user` and `assistant` and `content`\n","            must be non-empty. Each row may contain multiple messages, but the first\n","            and last author must be the `user`. An optional `context` field may be\n","            provided for each example in a chat dataset. If provided, the `context`\n","            will preprended to the message `content`. The `instruction` serves as\n","            the default context. (Useful if most messages use the same system-level\n","            context.) Any context provided in the example will override the default\n","            value.\n","          parameterType: STRING\n","        prompt_sequence_length:\n","          defaultValue: 512.0\n","          description: Maximum tokenized sequence length for input text. Higher values\n","            increase memory overhead. This value should be at most 8192. Default value\n","            is 512.\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        reinforcement_learning_rate_multiplier:\n","          defaultValue: 1.0\n","          description: Constant used to adjust the base learning rate used during\n","            reinforcement learning. Multiply by a number > 1 to increase the magnitude\n","            of updates applied at each training step or multiply by a number < 1 to\n","            decrease the magnitude of updates. Default value is 1.0.\n","          isOptional: true\n","          parameterType: NUMBER_DOUBLE\n","        reinforcement_learning_train_steps:\n","          defaultValue: 1000.0\n","          description: Number of reinforcement learning steps to perform when tuning\n","            a base model. Default value is 1000.\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        reward_lora_dim:\n","          defaultValue: 4.0\n","          description: The rank of the reward LoRA adapter. Full tuning is not support\n","            for the reward model. Default is 4.\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        reward_model_reference:\n","          description: Name of the reward model. The name should be in capitalized\n","            snake case format.\n","          parameterType: STRING\n","        rl_image_uri:\n","          description: Docker image URI to use for the reinforcement learning training\n","            job.\n","          parameterType: STRING\n","        target_sequence_length:\n","          defaultValue: 64.0\n","          description: Maximum tokenized sequence length for target text. Higher values\n","            increase memory overhead. This value should be at most 1024. Default value\n","            is 64.\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        tensorboard_resource_id:\n","          defaultValue: ''\n","          description: Optional tensorboard resource id in format `projects/{project_number}/locations/{location}/tensorboards/{tensorboard_id}`.\n","            If provided, tensorboard metrics will be uploaded to this location.\n","          isOptional: true\n","          parameterType: STRING\n","        tuning_location:\n","          description: The GCP region to run the custom job.\n","          parameterType: STRING\n","    outputDefinitions:\n","      parameters:\n","        output_adapter_path:\n","          description: Path to the trained model adapter if LoRA tuning was used.\n","          parameterType: STRING\n","        output_model_path:\n","          description: Path to the trained model checkpoint.\n","          parameterType: STRING\n","  comp-reinforcer:\n","    executorLabel: exec-reinforcer\n","    inputDefinitions:\n","      parameters:\n","        accelerator_count:\n","          description: Number of TPU accelerators.\n","          parameterType: NUMBER_INTEGER\n","        accelerator_type:\n","          description: Type of TPU accelerator. Can be either TPU_V2 or TPU_V3.\n","          parameterType: STRING\n","        batch_size:\n","          defaultValue: 64.0\n","          description: Number of examples in each finetuning step. Default is 64.\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        encryption_spec_key_name:\n","          defaultValue: ''\n","          description: 'Customer-managed encryption key. If this is set,\n","\n","            then all resources created by the CustomJob will be encrypted with the\n","\n","            provided encryption key. Note that this is not supported for TPU at the\n","\n","            moment.'\n","          isOptional: true\n","          parameterType: STRING\n","        image_uri:\n","          description: Location of reinforcement learning Docker image.\n","          parameterType: STRING\n","        input_dataset_path:\n","          description: Path to training dataset.\n","          parameterType: STRING\n","        input_preference_dataset_path:\n","          description: Path to preference dataset.\n","          parameterType: STRING\n","        input_reference_model_path:\n","          description: Path to the base model to fine tune.\n","          parameterType: STRING\n","        input_reward_adapter_path:\n","          description: Path to the reward model's LoRA adapter.\n","          parameterType: STRING\n","        input_reward_model_path:\n","          description: 'Path to the reward model to use during\n","\n","            reinforcement learning.'\n","          parameterType: STRING\n","        inputs_sequence_length:\n","          description: Maximum number of input tokens per row.\n","          parameterType: NUMBER_INTEGER\n","        kl_coeff:\n","          defaultValue: 0.1\n","          description: 'Coefficient for KL penalty. This regularizes the policy model\n","            and\n","\n","            penalizes if it diverges from its initial distribution. If set to 0, then\n","\n","            the reference LM is not loaded into memory.'\n","          isOptional: true\n","          parameterType: NUMBER_DOUBLE\n","        large_model_reference:\n","          description: 'Predefined model used to create the\n","\n","            ``input_reference_model``.'\n","          parameterType: STRING\n","        learning_rate_multiplier:\n","          defaultValue: 1.0\n","          description: 'Constant multiplied by the base learning rate used\n","\n","            to adjust the learning rate during reinforcement learning.'\n","          isOptional: true\n","          parameterType: NUMBER_DOUBLE\n","        location:\n","          description: Location used to run the job.\n","          parameterType: STRING\n","        lora_dim:\n","          defaultValue: 0.0\n","          description: 'The rank of the LoRA adapter. If >0, then use LoRA-tuning.\n","            If =0,\n","\n","            then use full-tuning.'\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        machine_type:\n","          description: 'The type of the machine to provision for the custom job. Must\n","\n","            be a valid GCE instance type and compatible with the accelerator type.'\n","          parameterType: STRING\n","        num_microbatches:\n","          defaultValue: 0.0\n","          description: 'Number of microbatches to break the total batch size into\n","\n","            during training. If <= 1, the model is trained on the full batch size\n","\n","            directly.'\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        project:\n","          description: Project used to run the job.\n","          parameterType: STRING\n","        reward_lora_dim:\n","          defaultValue: 4.0\n","          description: 'The rank of the Reward model LoRA adapter. Full tuning is\n","\n","            not support for the reward model. Default is 4.'\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        reward_model_reference:\n","          parameterType: STRING\n","        targets_sequence_length:\n","          description: Maximum number of target tokens per row.\n","          parameterType: NUMBER_INTEGER\n","        tensorboard_resource_id:\n","          defaultValue: ''\n","          description: 'Optional tensorboard resource id. Format:\n","\n","            `projects/{project_number}/locations/{location}/tensorboards/{tensorboard_id}`.\n","\n","            If provided, tensorboard metrics will be uploaded to this location.'\n","          isOptional: true\n","          parameterType: STRING\n","        train_split:\n","          defaultValue: train\n","          description: 'Name of the split in the input dataset that contains training\n","\n","            data. Default is ``''train''``.'\n","          isOptional: true\n","          parameterType: STRING\n","        train_steps:\n","          description: 'Number of training steps. These are the number of steps on\n","            top\n","\n","            of any steps used to train the base model.'\n","          parameterType: NUMBER_INTEGER\n","    outputDefinitions:\n","      artifacts:\n","        tensorboard_metrics:\n","          artifactType:\n","            schemaTitle: system.Artifact\n","            schemaVersion: 0.0.1\n","          description: Training stats (tensorboard) path.\n","      parameters:\n","        gcp_resources:\n","          description: 'GCP resources that can be used to track the custom finetuning\n","\n","            job.'\n","          parameterType: STRING\n","        output_adapter_path:\n","          description: 'Path to the trained model adapter if LoRA tuning was\n","\n","            used.'\n","          parameterType: STRING\n","        output_model_path:\n","          description: Path to the trained model checkpoint.\n","          parameterType: STRING\n","  comp-reward-model-graph:\n","    dag:\n","      outputs:\n","        parameters:\n","          reward_dataset_path:\n","            valueFromParameter:\n","              outputParameterKey: output_dataset_path\n","              producerSubtask: private-text-comparison-importer\n","          reward_model_adapter_path:\n","            valueFromParameter:\n","              outputParameterKey: output_adapter_path\n","              producerSubtask: reward-model-trainer\n","      tasks:\n","        preprocess-chat-dataset:\n","          cachingOptions:\n","            enableCache: true\n","          componentRef:\n","            name: comp-preprocess-chat-dataset\n","          inputs:\n","            parameters:\n","              dataset_type:\n","                runtimeValue:\n","                  constant: preference\n","              default_context:\n","                componentInputParameter: instruction\n","              input_dataset_uri:\n","                componentInputParameter: preference_dataset\n","              large_model_reference:\n","                componentInputParameter: large_model_reference\n","          taskInfo:\n","            name: Preprocess Prompt Dataset\n","        private-text-comparison-importer:\n","          cachingOptions: {}\n","          componentRef:\n","            name: comp-private-text-comparison-importer\n","          dependentTasks:\n","          - preprocess-chat-dataset\n","          inputs:\n","            parameters:\n","              choice_field_name:\n","                runtimeValue:\n","                  constant: choice\n","              comma_separated_candidates_field_names:\n","                componentInputParameter: comma_separated_candidates_field_names\n","              encryption_spec_key_name:\n","                componentInputParameter: encryption_spec_key_name\n","              input_text:\n","                taskOutputParameter:\n","                  outputParameterKey: processed_dataset_uri\n","                  producerTask: preprocess-chat-dataset\n","              inputs_field_name:\n","                runtimeValue:\n","                  constant: input_text\n","              instruction:\n","                componentInputParameter: instruction\n","              large_model_reference:\n","                componentInputParameter: reward_model_reference\n","              location:\n","                componentInputParameter: location\n","              project:\n","                componentInputParameter: project\n","              split:\n","                runtimeValue:\n","                  constant: train\n","          taskInfo:\n","            name: Import Preference Dataset\n","        private-text-comparison-importer-2:\n","          cachingOptions: {}\n","          componentRef:\n","            name: comp-private-text-comparison-importer-2\n","          inputs:\n","            parameters:\n","              choice_field_name:\n","                runtimeValue:\n","                  constant: choice\n","              comma_separated_candidates_field_names:\n","                componentInputParameter: comma_separated_candidates_field_names\n","              encryption_spec_key_name:\n","                componentInputParameter: encryption_spec_key_name\n","              input_text:\n","                componentInputParameter: eval_dataset\n","              inputs_field_name:\n","                runtimeValue:\n","                  constant: input_text\n","              instruction:\n","                componentInputParameter: instruction\n","              large_model_reference:\n","                componentInputParameter: reward_model_reference\n","              location:\n","                componentInputParameter: location\n","              project:\n","                componentInputParameter: project\n","              split:\n","                runtimeValue:\n","                  constant: train\n","          taskInfo:\n","            name: Import Preference Eval Dataset\n","        reward-model-trainer:\n","          cachingOptions: {}\n","          componentRef:\n","            name: comp-reward-model-trainer\n","          dependentTasks:\n","          - private-text-comparison-importer\n","          - private-text-comparison-importer-2\n","          inputs:\n","            parameters:\n","              accelerator_count:\n","                componentInputParameter: accelerator_count\n","              accelerator_type:\n","                componentInputParameter: accelerator_type\n","              batch_size:\n","                componentInputParameter: batch_size\n","              encryption_spec_key_name:\n","                componentInputParameter: encryption_spec_key_name\n","              eval_dataset_path:\n","                taskOutputParameter:\n","                  outputParameterKey: output_dataset_path\n","                  producerTask: private-text-comparison-importer-2\n","              image_uri:\n","                componentInputParameter: reward_model_image_uri\n","              input_dataset_path:\n","                taskOutputParameter:\n","                  outputParameterKey: output_dataset_path\n","                  producerTask: private-text-comparison-importer\n","              input_model_path:\n","                componentInputParameter: reward_model_path\n","              inputs_sequence_length:\n","                componentInputParameter: prompt_sequence_length\n","              large_model_reference:\n","                componentInputParameter: reward_model_reference\n","              learning_rate_multiplier:\n","                componentInputParameter: reward_model_learning_rate_multiplier\n","              location:\n","                componentInputParameter: tuning_location\n","              lora_dim:\n","                componentInputParameter: lora_dim\n","              machine_type:\n","                componentInputParameter: machine_type\n","              num_microbatches:\n","                componentInputParameter: num_microbatches\n","              project:\n","                componentInputParameter: project\n","              targets_sequence_length:\n","                componentInputParameter: target_sequence_length\n","              tensorboard_resource_id:\n","                componentInputParameter: tensorboard_resource_id\n","              train_steps:\n","                componentInputParameter: reward_model_train_steps\n","          taskInfo:\n","            name: Reward Model Trainer\n","    inputDefinitions:\n","      parameters:\n","        accelerator_count:\n","          description: The number of accelerator.\n","          parameterType: NUMBER_INTEGER\n","        accelerator_type:\n","          description: Specific accelerator type for the custom job.\n","          parameterType: STRING\n","        batch_size:\n","          defaultValue: 64.0\n","          description: Number of examples in each finetuning step. Default is 64.\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        comma_separated_candidates_field_names:\n","          description: Comma separated list of fields that contain candidate text,\n","            e.g. ``'field_1,field_2,field_3'``.\n","          parameterType: STRING\n","        encryption_spec_key_name:\n","          defaultValue: ''\n","          description: Customer-managed encryption key. If this is set, then all resources\n","            created by the CustomJob will be encrypted with the provided encryption\n","            key. Note that this is not supported for TPU at the moment.\n","          isOptional: true\n","          parameterType: STRING\n","        eval_dataset:\n","          isOptional: true\n","          parameterType: STRING\n","        instruction:\n","          description: This field lets the model know what task it needs to perform.\n","            Base models have been trained over a large set of varied instructions.\n","            You can give a simple and intuitive description of the task and the model\n","            will follow it, e.g. \"Classify this movie review as positive or negative\"\n","            or \"Translate this sentence to Danish\". Do not specify this if your dataset\n","            already prepends the instruction to the inputs field.\n","          isOptional: true\n","          parameterType: STRING\n","        large_model_reference:\n","          description: Name of the base model. Supported values are `text-bison@001`,\n","            `t5-small`, `t5-large`, `t5-xl` and `t5-xxl`. `text-bison@001` and `t5-small`\n","            are supported in `us-central1` and `europe-west4`. `t5-large`, `t5-xl`\n","            and `t5-xxl` are only supported in `europe-west4`.\n","          parameterType: STRING\n","        location:\n","          defaultValue: '{{$.pipeline_google_cloud_location}}'\n","          description: Location used to run non-tuning components, i.e. components\n","            that do not require accelerators. If not specified the location used to\n","            run the pipeline will be used.\n","          isOptional: true\n","          parameterType: STRING\n","        lora_dim:\n","          defaultValue: 4.0\n","          description: The rank of the LoRA adapter. If >0, then use LoRA-tuning.\n","            Full tuning is not supported for the reward model. Default is 4.\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        machine_type:\n","          description: The type of the machine to provision for the custom job. Must\n","            be a valid GCE instance type and compatible with the accelerator type.\n","          parameterType: STRING\n","        num_microbatches:\n","          defaultValue: 0.0\n","          description: The number of microbatches to break the total batch size into\n","            during training.\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        preference_dataset:\n","          description: Cloud storage path to a human preference JSONL dataset used\n","            to train a reward model. Each example in a preference dataset must contain\n","            `candidate_0` and `candidate_1` fields that contain candidate responses,\n","            `choice` that specifies the preferred candidate and either `input_text`\n","            (if tuning a text model) or `messages` (if tuning a chat model). Chat\n","            datasets must contain at least 1 message in a `messages` field. Each message\n","            must be valid JSON that contains `author` and `content` fields, where\n","            valid `author` values are `user` and `assistant` and `content` must be\n","            non-empty. Each row may contain multiple messages, but the first and last\n","            author must be the `user`. An optional `context` field may be provided\n","            for each example in a chat dataset. If provided, the `context` will preprended\n","            to the message `content`. The `instruction` serves as the default context.\n","            (Useful if most messages use the same system-level context.) Any context\n","            provided in the example will override the default value.\n","          parameterType: STRING\n","        project:\n","          defaultValue: '{{$.pipeline_google_cloud_project_id}}'\n","          description: Project used to run custom jobs. If not specified the project\n","            used to run the pipeline will be used.\n","          isOptional: true\n","          parameterType: STRING\n","        prompt_sequence_length:\n","          defaultValue: 512.0\n","          description: Maximum tokenized sequence length for input text. Higher values\n","            increase memory overhead. This value should be at most 8192. Default value\n","            is 512.\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        reward_model_image_uri:\n","          description: Docker image URI to use for the reward model training job.\n","          parameterType: STRING\n","        reward_model_learning_rate_multiplier:\n","          defaultValue: 1.0\n","          description: Constant used to adjust the base learning rate used when training\n","            a reward model. Multiply by a number > 1 to increase the magnitude of\n","            updates applied at each training step or multiply by a number < 1 to decrease\n","            the magnitude of updates. Default value is 1.0.\n","          isOptional: true\n","          parameterType: NUMBER_DOUBLE\n","        reward_model_path:\n","          description: The model checkpoint path for the reward model.\n","          parameterType: STRING\n","        reward_model_reference:\n","          description: Name of the base model. The name should be in capitalized snake\n","            case format.\n","          parameterType: STRING\n","        reward_model_train_steps:\n","          defaultValue: 1000.0\n","          description: Number of steps to use when training a reward model. Default\n","            value is 1000.\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        target_sequence_length:\n","          defaultValue: 64.0\n","          description: ' Maximum tokenized sequence length for target text. Higher\n","            values increase memory overhead. This value should be at most 1024. Default\n","            value is 64.'\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        tensorboard_resource_id:\n","          defaultValue: ''\n","          description: Optional tensorboard resource id in format `projects/{project_number}/locations/{location}/tensorboards/{tensorboard_id}`.\n","            If provided, tensorboard metrics will be uploaded to this location.\n","          isOptional: true\n","          parameterType: STRING\n","        tuning_location:\n","          description: The GCP region to run the custom job.\n","          parameterType: STRING\n","    outputDefinitions:\n","      parameters:\n","        reward_dataset_path:\n","          description: Preference dataset use for tuning the reward model.\n","          parameterType: STRING\n","        reward_model_adapter_path:\n","          description: Path to the output LoRA adapter.\n","          parameterType: STRING\n","  comp-reward-model-trainer:\n","    executorLabel: exec-reward-model-trainer\n","    inputDefinitions:\n","      parameters:\n","        accelerator_count:\n","          description: Number of TPU accelerators.\n","          parameterType: NUMBER_INTEGER\n","        accelerator_type:\n","          description: Type of TPU accelerator. Can be either TPU_V2 or TPU_V3.\n","          parameterType: STRING\n","        batch_size:\n","          defaultValue: 64.0\n","          description: Number of examples in each finetuning step. Default is 64.\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        encryption_spec_key_name:\n","          defaultValue: ''\n","          description: 'Customer-managed encryption key. If this is set,\n","\n","            then all resources created by the CustomJob will be encrypted with the\n","\n","            provided encryption key. Note that this is not supported for TPU at the\n","\n","            moment.'\n","          isOptional: true\n","          parameterType: STRING\n","        eval_dataset_path:\n","          defaultValue: ''\n","          description: 'Path to eval dataset to use during the reward model\n","\n","            training.'\n","          isOptional: true\n","          parameterType: STRING\n","        image_uri:\n","          description: Location of reward model Docker image.\n","          parameterType: STRING\n","        input_dataset_path:\n","          description: Path to dataset to use to train a reward model.\n","          parameterType: STRING\n","        input_model_path:\n","          description: Path to the base model to fine tune.\n","          parameterType: STRING\n","        inputs_sequence_length:\n","          description: Maximum number of input tokens per row.\n","          parameterType: NUMBER_INTEGER\n","        large_model_reference:\n","          description: Predefined model used to create the ``input_model``.\n","          parameterType: STRING\n","        learning_rate_multiplier:\n","          defaultValue: 1.0\n","          description: 'Constant multiplied by the base learning rate used\n","\n","            to adjust the learning rate when training a reward model.'\n","          isOptional: true\n","          parameterType: NUMBER_DOUBLE\n","        location:\n","          description: Location used to run the job.\n","          parameterType: STRING\n","        lora_dim:\n","          defaultValue: 4.0\n","          description: 'The rank of the LoRA adapter. If >0, then use LoRA-tuning.\n","            If =0,\n","\n","            then use full-tuning.'\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        machine_type:\n","          description: 'The type of the machine to provision for the custom job. Must\n","\n","            be a valid GCE instance type and compatible with the accelerator type.'\n","          parameterType: STRING\n","        num_microbatches:\n","          defaultValue: 0.0\n","          description: 'Number of microbatches to break the total batch size into\n","\n","            during training. If <= 1, the model is trained on the full batch size\n","\n","            directly.'\n","          isOptional: true\n","          parameterType: NUMBER_INTEGER\n","        project:\n","          description: Project used to run the job.\n","          parameterType: STRING\n","        targets_sequence_length:\n","          description: Maximum number of target tokens per row.\n","          parameterType: NUMBER_INTEGER\n","        tensorboard_resource_id:\n","          defaultValue: ''\n","          description: 'Optional tensorboard resource id. Format:\n","\n","            `projects/{project_number}/locations/{location}/tensorboards/{tensorboard_id}`.\n","\n","            If provided, tensorboard metrics will be uploaded to this location.'\n","          isOptional: true\n","          parameterType: STRING\n","        train_split:\n","          defaultValue: train\n","          description: 'Name of the split in the input dataset that contains training\n","\n","            data. Default is ``''train''``.'\n","          isOptional: true\n","          parameterType: STRING\n","        train_steps:\n","          description: 'Number of training steps. These are the number of steps on\n","            top\n","\n","            of any steps used to train the base model.'\n","          parameterType: NUMBER_INTEGER\n","    outputDefinitions:\n","      artifacts:\n","        tensorboard_metrics:\n","          artifactType:\n","            schemaTitle: system.Artifact\n","            schemaVersion: 0.0.1\n","          description: Training stats (tensorboard) path.\n","      parameters:\n","        gcp_resources:\n","          description: 'GCP resources that can be used to track the custom finetuning\n","\n","            job.'\n","          parameterType: STRING\n","        output_adapter_path:\n","          description: Trained reward LoRA adapter.\n","          parameterType: STRING\n","  comp-rlhf-preprocessor:\n","    executorLabel: exec-rlhf-preprocessor\n","    inputDefinitions:\n","      parameters:\n","        accelerator_type:\n","          description: Specific accelerator type for the job.\n","          parameterType: STRING\n","        artifact_registry:\n","          description: Registry that contains Docker images.\n","          parameterType: STRING\n","        deploy_model:\n","          defaultValue: true\n","          description: Whether to deploy the model.\n","          isOptional: true\n","          parameterType: BOOLEAN\n","        evaluation_dataset:\n","          defaultValue: ''\n","          description: Path to evaluation data.\n","          isOptional: true\n","          parameterType: STRING\n","        image_uri:\n","          defaultValue: us-docker.pkg.dev/vertex-ai-restricted/rlhf/refined_cpu:20240623_1707\n","          description: Docker image URI to use for the custom job.\n","          isOptional: true\n","          parameterType: STRING\n","        input_reference_model_path:\n","          defaultValue: ''\n","          isOptional: true\n","          parameterType: STRING\n","        large_model_reference:\n","          description: The model for fine tuning.\n","          parameterType: STRING\n","        location:\n","          description: Region that contains the artifact registry.\n","          parameterType: STRING\n","        model_display_name:\n","          defaultValue: ''\n","          description: Display name of the model.\n","          isOptional: true\n","          parameterType: STRING\n","        project:\n","          description: Project that contains the artifact registry.\n","          parameterType: STRING\n","        tag:\n","          description: Image tag.\n","          parameterType: STRING\n","        tensorboard_resource_id:\n","          defaultValue: ''\n","          description: TensorBoard resource id.\n","          isOptional: true\n","          parameterType: STRING\n","        upload_location:\n","          defaultValue: ''\n","          description: Region where the model will be uploaded.\n","          isOptional: true\n","          parameterType: STRING\n","        use_experimental_image:\n","          defaultValue: false\n","          description: ' Whether to use refined experimental image.'\n","          isOptional: true\n","          parameterType: BOOLEAN\n","        use_test_spec:\n","          description: Whether to use a lower resource machine for testing.\n","          parameterType: BOOLEAN\n","    outputDefinitions:\n","      parameters:\n","        gcp_resources:\n","          description: GCP resources that can be used to track the custom job.\n","          parameterType: STRING\n","        has_inference_dataset:\n","          description: Whether inference data are provided.\n","          parameterType: BOOLEAN\n","        has_tensorboard_id:\n","          description: Whether a tensorboard id is provided.\n","          parameterType: BOOLEAN\n","        metadata_accelerator_count:\n","          description: The number of accelerator.\n","          parameterType: NUMBER_INTEGER\n","        metadata_accelerator_type:\n","          description: Specific accelerator type for the custom job.\n","          parameterType: STRING\n","        metadata_candidate_columns_string:\n","          parameterType: STRING\n","        metadata_deploy_model:\n","          description: Whether to deploy the model.\n","          parameterType: BOOLEAN\n","        metadata_large_model_reference:\n","          parameterType: STRING\n","        metadata_machine_type:\n","          description: The type of the machine to provision for the custom job.\n","          parameterType: STRING\n","        metadata_model_display_name:\n","          description: Display name of the model.\n","          parameterType: STRING\n","        metadata_num_microbatches:\n","          description: 'Number of microbatches to break the total batch\n","\n","            size into during training.'\n","          parameterType: NUMBER_INTEGER\n","        metadata_reference_model_path:\n","          parameterType: STRING\n","        metadata_refined_image_uri:\n","          description: Docker image URI to use for the custom job.\n","          parameterType: STRING\n","        metadata_reward_model_path:\n","          parameterType: STRING\n","        metadata_reward_model_reference:\n","          parameterType: STRING\n","        metadata_tuning_location:\n","          description: The GCP region to run the custom job.\n","          parameterType: STRING\n","        metadata_upload_location:\n","          description: Regional endpoint.\n","          parameterType: STRING\n","        metadata_upload_model:\n","          description: Whether to upload the model.\n","          parameterType: BOOLEAN\n","  comp-validate-pipeline:\n","    executorLabel: exec-validate-pipeline\n","    inputDefinitions:\n","      parameters:\n","        accelerator_type:\n","          defaultValue: ''\n","          description: 'One of ''TPU'' or ''GPU''. If ''TPU'' is specified, tuning\n","\n","            components run in europe-west4. Otherwise tuning components run in\n","\n","            us-central1 on GPUs. Default is ''GPU''.'\n","          isOptional: true\n","          parameterType: STRING\n","        encryption_spec_key_name:\n","          defaultValue: ''\n","          description: If set, CMEK support will be validated.\n","          isOptional: true\n","          parameterType: STRING\n","        eval_dataset:\n","          description: 'Optional Cloud storage path to an evaluation dataset. The\n","\n","            format should match that of the preference dataset.'\n","          isOptional: true\n","          parameterType: STRING\n","        location:\n","          description: 'Location used to run non-tuning components, i.e. components\n","\n","            that do not require accelerators. If not specified the location used\n","\n","            to run the pipeline will be used.'\n","          parameterType: STRING\n","    outputDefinitions:\n","      parameters:\n","        reward_model_eval_dataset:\n","          parameterType: STRING\n","deploymentSpec:\n","  executors:\n","    exec-bulk-inferrer:\n","      container:\n","        args:\n","        - --type\n","        - CustomJob\n","        - --payload\n","        - '{\"display_name\": \"BulkInferrer\", \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\":\n","          \"1\", \"machine_spec\": {\"machine_type\": \"{{$.inputs.parameters[''machine_type'']}}\",\n","          \"accelerator_type\": \"{{$.inputs.parameters[''accelerator_type'']}}\", \"accelerator_count\":\n","          {{$.inputs.parameters[''accelerator_count'']}}}, \"container_spec\": {\"image_uri\":\n","          \"{{$.inputs.parameters[''image_uri'']}}\", \"args\": [\"--app_name=bulk_inferrer\",\n","          \"--input_model={{$.inputs.parameters[''input_model'']}}\", \"--input_dataset={{$.inputs.parameters[''input_dataset_path'']}}\",\n","          \"--dataset_split={{$.inputs.parameters[''dataset_split'']}}\", \"--large_model_reference={{$.inputs.parameters[''large_model_reference'']}}\",\n","          \"--inputs_sequence_length={{$.inputs.parameters[''inputs_sequence_length'']}}\",\n","          \"--targets_sequence_length={{$.inputs.parameters[''targets_sequence_length'']}}\",\n","          \"--sampling_strategy={{$.inputs.parameters[''sampling_strategy'']}}\", \"--output_prediction={{$.outputs.parameters[''output_prediction''].output_file}}\",\n","          \"--output_prediction_gcs_path={{$.outputs.parameters[''output_prediction_gcs_path''].output_file}}\"]}}]},\n","          \"encryption_spec\": {\"kms_key_name\": \"{{$.inputs.parameters[''encryption_spec_key_name'']}}\"}}'\n","        - --project\n","        - '{{$.inputs.parameters[''project'']}}'\n","        - --location\n","        - '{{$.inputs.parameters[''location'']}}'\n","        - --gcp_resources\n","        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'\n","        command:\n","        - python3\n","        - -u\n","        - -m\n","        - google_cloud_pipeline_components.container.v1.custom_job.launcher\n","        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.16.1\n","    exec-deploy-llm-model:\n","      container:\n","        args:\n","        - --executor_input\n","        - '{{$}}'\n","        - --function_to_execute\n","        - deploy_llm_model\n","        command:\n","        - sh\n","        - -ec\n","        - 'program_path=$(mktemp -d)\n","\n","\n","          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n","\n","          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n","\n","          '\n","        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\n","          \\ *\\n\\ndef deploy_llm_model(\\n    project: str,\\n    location: str,\\n  \\\n","          \\  model_resource_name: str,\\n    display_name: str,\\n    regional_endpoint:\\\n","          \\ str,\\n    endpoint_resource_name: dsl.OutputPath(str),\\n    create_endpoint_gcp_resources:\\\n","          \\ dsl.OutputPath(str),\\n    deploy_model_gcp_resources: dsl.OutputPath(str),\\n\\\n","          \\    encryption_spec_key_name: str = '',\\n    service_account: str = '',\\n\\\n","          \\    deploy_model: bool = True,\\n):\\n  \\\"\\\"\\\"Creates a vertex endpoint and\\\n","          \\ deploy the specified model.\\n\\n  Args:\\n      project: Name of the GCP\\\n","          \\ project.\\n      location: Location for model upload and deployment.\\n\\\n","          \\      model_resource_name: Path to the created Model on Model Registry.\\n\\\n","          \\      display_name: Name of the model (shown in Model Registry).\\n    \\\n","          \\  regional_endpoint: Regional API endpoint.\\n      encryption_spec_key_name:\\\n","          \\ Customer-managed encryption key.\\n      service_account: If set, then\\\n","          \\ a custom service account will be used.\\n      deploy_model: Whether to\\\n","          \\ deploy the model to an endpoint. Default is\\n        ``True``. If ``False``,\\\n","          \\ the model will not be deployed and output\\n        artifacts will contain\\\n","          \\ empty strings.\\n\\n  Returns:\\n      endpoint_resource_name: Path to the\\\n","          \\ created endpoint on Online Prediction.\\n      create_endpoint_gcp_resources:\\\n","          \\ Serialized JSON of GCP resources for\\n          creating an endpoint.\\n\\\n","          \\      deploy_model_gcp_resources: Serialized JSON of GCP resources for\\\n","          \\ deploying\\n          the model.\\n  \\\"\\\"\\\"\\n  import json\\n  import logging\\n\\\n","          \\  import os\\n  import sys\\n  from typing import Any, Dict\\n\\n  try:\\n \\\n","          \\   from google_cloud_pipeline_components.container.v1.gcp_launcher import\\\n","          \\ lro_remote_runner\\n  except ImportError:\\n    from google_cloud_pipeline_components.container.v1.gcp_launcher\\\n","          \\ import lro_remote_runner\\n\\n  def run_lro_remote_runner(\\n      url: str,\\\n","          \\ payload: Dict[str, Any], gcp_resources: str\\n  ) -> Any:\\n    remote_runner\\\n","          \\ = lro_remote_runner.LroRemoteRunner(location)\\n    lro = remote_runner.create_lro(url,\\\n","          \\ json.dumps(payload), gcp_resources)\\n    return remote_runner.poll_lro(lro=lro)\\n\\\n","          \\n  try:\\n    os.makedirs(os.path.dirname(endpoint_resource_name), exist_ok=True)\\n\\\n","          \\n    if not deploy_model:\\n      with open(endpoint_resource_name, 'w')\\\n","          \\ as fout:\\n        fout.write('')\\n      return\\n\\n    regional_endpoint\\\n","          \\ = regional_endpoint.rstrip('/')\\n\\n    create_endpoint_payload = {\\n \\\n","          \\       'displayName': display_name,\\n    }\\n\\n    pipeline_labels_str =\\\n","          \\ os.getenv('VERTEX_AI_PIPELINES_RUN_LABELS')\\n    if pipeline_labels_str:\\n\\\n","          \\      create_endpoint_payload['labels'] = json.loads(pipeline_labels_str)\\n\\\n","          \\n    if encryption_spec_key_name:\\n      create_endpoint_payload['encryption_spec']\\\n","          \\ = {\\n          'kms_key_name': encryption_spec_key_name\\n      }\\n\\n \\\n","          \\   create_endpoint_lro = run_lro_remote_runner(\\n        url=(\\n      \\\n","          \\      f'{regional_endpoint}/projects/{project}/locations/{location}'\\n\\\n","          \\            '/endpoints'\\n        ),\\n        payload=create_endpoint_payload,\\n\\\n","          \\        gcp_resources=create_endpoint_gcp_resources,\\n    )\\n\\n    response_endpoint\\\n","          \\ = create_endpoint_lro['response']['name']\\n    with open(endpoint_resource_name,\\\n","          \\ 'w') as fout:\\n      fout.write(response_endpoint)\\n\\n    logging.info(\\n\\\n","          \\        'Endpoint created successfully. Deploying model %s to endpoint',\\n\\\n","          \\        model_resource_name,\\n    )\\n\\n    deploy_model_payload = {\\n \\\n","          \\       'deployedModel': {\\n            'model': model_resource_name,\\n\\\n","          \\            'displayName': display_name,\\n            'automaticResources':\\\n","          \\ {'minReplicaCount': 1, 'maxReplicaCount': 1},\\n        }\\n    }\\n    if\\\n","          \\ service_account:\\n      deploy_model_payload['deployedModel']['service_account']\\\n","          \\ = service_account\\n\\n    _ = run_lro_remote_runner(\\n        url=f'{regional_endpoint}/{response_endpoint}:deployModel',\\n\\\n","          \\        payload=deploy_model_payload,\\n        gcp_resources=deploy_model_gcp_resources,\\n\\\n","          \\    )\\n\\n    logging.info('Model deployed successfully!')\\n  except Exception\\\n","          \\ as e:  # pylint: disable=broad-exception-caught\\n    if isinstance(e,\\\n","          \\ ValueError):\\n      raise\\n    logging.exception(str(e))\\n    sys.exit(13)\\n\\\n","          \\n\"\n","        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.16.1\n","    exec-infer-preprocessor:\n","      container:\n","        args:\n","        - --type\n","        - CustomJob\n","        - --payload\n","        - '{\"display_name\": \"infer_preprocessor\", \"job_spec\": {\"worker_pool_specs\":\n","          [{\"replica_count\": \"1\", \"machine_spec\": {\"machine_type\": \"n1-standard-4\"},\n","          \"container_spec\": {\"image_uri\": \"{{$.inputs.parameters[''image_uri'']}}\",\n","          \"args\": [\"--app_name=infer_preprocessor\", \"--large_model_reference={{$.inputs.parameters[''large_model_reference'']}}\",\n","          \"--input_reference_model_path={{$.inputs.parameters[''input_reference_model_path'']}}\",\n","          \"--accelerator_type={{$.inputs.parameters[''accelerator_type'']}}\", \"--use_test_spec={{$.inputs.parameters[''use_test_spec'']}}\",\n","          \"--project={{$.inputs.parameters[''project'']}}\", \"--location={{$.inputs.parameters[''location'']}}\",\n","          \"--artifact_registry={{$.inputs.parameters[''artifact_registry'']}}\", \"--tag={{$.inputs.parameters[''tag'']}}\",\n","          \"--use_experimental_image={{$.inputs.parameters[''use_experimental_image'']}}\",\n","          \"--instruction={{$.inputs.parameters[''instruction'']}}\", \"--metadata_large_model_reference_path={{$.outputs.parameters[''metadata_large_model_reference''].output_file}}\",\n","          \"--metadata_reference_model_path_path={{$.outputs.parameters[''metadata_reference_model_path''].output_file}}\",\n","          \"--metadata_reward_model_reference_path={{$.outputs.parameters[''metadata_reward_model_reference''].output_file}}\",\n","          \"--metadata_reward_model_path_path={{$.outputs.parameters[''metadata_reward_model_path''].output_file}}\",\n","          \"--metadata_machine_type_path={{$.outputs.parameters[''metadata_machine_type''].output_file}}\",\n","          \"--metadata_tuning_location_path={{$.outputs.parameters[''metadata_tuning_location''].output_file}}\",\n","          \"--metadata_accelerator_type_path={{$.outputs.parameters[''metadata_accelerator_type''].output_file}}\",\n","          \"--metadata_accelerator_count_path={{$.outputs.parameters[''metadata_accelerator_count''].output_file}}\",\n","          \"--metadata_instruction_path={{$.outputs.parameters[''metadata_instruction''].output_file}}\",\n","          \"--metadata_refined_image_uri_path={{$.outputs.parameters[''metadata_refined_image_uri''].output_file}}\"]}}]}}'\n","        - --project\n","        - '{{$.pipeline_google_cloud_project_id}}'\n","        - --location\n","        - '{{$.pipeline_google_cloud_location}}'\n","        - --gcp_resources\n","        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'\n","        command:\n","        - python3\n","        - -u\n","        - -m\n","        - google_cloud_pipeline_components.container.v1.custom_job.launcher\n","        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.16.1\n","    exec-preprocess-chat-dataset:\n","      container:\n","        args:\n","        - --executor_input\n","        - '{{$}}'\n","        - --function_to_execute\n","        - preprocess_chat_dataset\n","        command:\n","        - sh\n","        - -ec\n","        - 'program_path=$(mktemp -d)\n","\n","\n","          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n","\n","          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n","\n","          '\n","        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\n","          \\ *\\n\\ndef preprocess_chat_dataset(\\n    large_model_reference: str,\\n \\\n","          \\   input_dataset_uri: str,\\n    dataset_type: str,\\n    processed_dataset:\\\n","          \\ dsl.OutputPath(dsl.Artifact),  # pytype: disable=invalid-annotation\\n\\\n","          \\    processed_dataset_uri: dsl.OutputPath(str),  # pytype: disable=invalid-annotation\\n\\\n","          \\    default_context: str = '',\\n    allow_local_files: bool = False,\\n\\\n","          ):  # pylint: disable=g-doc-args\\n  # fmt: off\\n  \\\"\\\"\\\"Preprocesses datasets\\\n","          \\ before tokenization.\\n\\n  For text datasets, this is a no-op.\\n\\n  Args:\\n\\\n","          \\    large_model_reference: Name of the base model. Supported values are\\\n","          \\ `text-bison@001`, `chat-bison@001`, `t5-small`, `t5-large`, `t5-xl` and\\\n","          \\ `t5-xxl`. `text-bison@001`, `chat-bison@001` and `t5-small` are supported\\\n","          \\ in ``us-central1` and `europe-west4`. `t5-large`, `t5-xl` and `t5-xxl`\\\n","          \\ are only supported in `europe-west4`.\\n    input_dataset_uri: Path to\\\n","          \\ an unprocessed JSONL dataset.\\n    default_context: Default context to\\\n","          \\ apply to each example if a chat model is specified.\\n    allow_local_files:\\\n","          \\ Whether input URIs can specify local file paths.\\n    is_prompt_dataset:\\\n","          \\ Whether the input dataset contains prompts for inference. In this case,\\\n","          \\ the last author in `messages` should be the `user`, and the output dataset\\\n","          \\ will only contain `input_text`.\\n\\n  Returns:\\n    processed_dataset:\\\n","          \\ Processed chat dataset. Each example will contain fields `input_text`,\\\n","          \\ and if the input dataset is not a prompt dataset example will also contain\\\n","          \\ `output_text`.\\n    processed_dataset_uri: String pattern that can be\\\n","          \\ used to find the processed dataset in downstream components.\\n\\n  \\\"\\\"\\\n","          \\\"\\n  # fmt: on\\n  # pylint: disable=g-import-not-at-top\\n  import dataclasses\\n\\\n","          \\  import json\\n  import os\\n  from typing import Any, Callable, List, Mapping\\n\\\n","          \\  import apache_beam as beam\\n  # pylint: enable=g-import-not-at-top\\n\\n\\\n","          \\  # [ Define helper methods and classes for preprocessing\\n  # pylint:\\\n","          \\ disable=invalid-name\\n  INPUT_TEXT_KEY = 'input_text'\\n  OUTPUT_TEXT_KEY\\\n","          \\ = 'output_text'\\n  CONTEXT_KEY = 'context'\\n  MESSAGES_KEY = 'messages'\\n\\\n","          \\  CANDIDATE_0_KEY = 'candidate_0'\\n  CANDIDATE_1_KEY = 'candidate_1'\\n\\\n","          \\  CHOICE_KEY = 'choice'\\n  AUTHOR_KEY = 'author'\\n  CONTENT_KEY = 'content'\\n\\\n","          \\  AUTHOR_USER = 'user'\\n  AUTHOR_ASSISTANT = 'assistant'\\n  VALID_AUTHORS\\\n","          \\ = {AUTHOR_USER, AUTHOR_ASSISTANT}\\n  SUPERVISED_DATASET = 'supervised'\\n\\\n","          \\  PROMPT_DATASET = 'prompt'\\n  PREFERENCE_DATASET = 'preference'\\n  VALID_DATASETS\\\n","          \\ = {SUPERVISED_DATASET, PROMPT_DATASET, PREFERENCE_DATASET}\\n  # pylint:\\\n","          \\ enable=invalid-name\\n\\n  @dataclasses.dataclass\\n  class PromptSchema:\\n\\\n","          \\    global_prefix: str\\n    user_prefix: str\\n    user_postfix: str\\n \\\n","          \\   assistant_prefix: str\\n    assistant_postfix: str\\n    get_system_message:\\\n","          \\ Callable[[str], str]  # pytype: disable=invalid-annotation\\n\\n  def _get_chat_bison_001_system_message(context:\\\n","          \\ str) -> str:\\n    return f'[SYSTEM]:{context}\\\\n\\\\n' if context else ''\\n\\\n","          \\n  chat_bison_001_schema = PromptSchema(\\n      global_prefix=(\\n     \\\n","          \\     'Only answer after [assistant] and never reply as [user]:\\\\n'\\n  \\\n","          \\    ),\\n      get_system_message=_get_chat_bison_001_system_message,\\n\\\n","          \\      user_prefix='[user]:',\\n      user_postfix='\\\\n',\\n      assistant_prefix='[assistant]:',\\n\\\n","          \\      assistant_postfix='\\\\n',\\n  )\\n\\n  def _get_chat_llama_system_message(context:\\\n","          \\ str) -> str:\\n    return f'<<SYS>>\\\\n{context}\\\\n<</SYS>>\\\\n\\\\n' if context\\\n","          \\ else ''\\n\\n  chat_llama_schema = PromptSchema(\\n      global_prefix='<s>[INST]\\\n","          \\ ',\\n      get_system_message=_get_chat_llama_system_message,\\n      user_prefix='',\\n\\\n","          \\      user_postfix=' [/INST]',\\n      assistant_prefix=' ',\\n      assistant_postfix='</s><s>[INST]\\\n","          \\ ',\\n  )\\n\\n  MODEL_TO_SCHEMA_MAPPING = {  # pylint: disable=invalid-name\\n\\\n","          \\      'chat-bison@001': chat_bison_001_schema,\\n      'llama-2-7b-chat':\\\n","          \\ chat_llama_schema,\\n      'llama-2-13b-chat': chat_llama_schema,\\n  }\\n\\\n","          \\n  def get_gcs_path(input_path: str, allow_local_files: bool) -> str:\\n\\\n","          \\    \\\"\\\"\\\"Gets the /gcs/ path for a given URI.\\\"\\\"\\\"\\n    if input_path.startswith('gs://'):\\n\\\n","          \\      return input_path.replace('gs://', '/gcs/', 1)\\n    elif input_path.startswith('/gcs/')\\\n","          \\ or allow_local_files:\\n      return input_path\\n    else:\\n      raise\\\n","          \\ ValueError(\\n          f'Invalid Cloud storage URI {input_path}. '\\n \\\n","          \\         'Must start with `gs://` or `/gcs/`.'\\n      )\\n\\n  def get_gs_path(input_path:\\\n","          \\ str, allow_local_files: bool) -> str:\\n    \\\"\\\"\\\"Gets the gs:// path for\\\n","          \\ a given URI.\\\"\\\"\\\"\\n    if input_path.startswith('/gcs/'):\\n      return\\\n","          \\ input_path.replace('/gcs/', 'gs://', 1)\\n    elif input_path.startswith('gs://')\\\n","          \\ or allow_local_files:\\n      return input_path\\n    else:\\n      raise\\\n","          \\ ValueError(\\n          f'Invalid Cloud storage URI {input_path}. '\\n \\\n","          \\         'Must start with `gs://` or `/gcs/`.'\\n      )\\n\\n  class JsonCoder(beam.coders.Coder):\\n\\\n","          \\    \\\"\\\"\\\"A coder that encodes/decodes lines as JSON strings.\\\"\\\"\\\"\\n\\n\\\n","          \\    def encode(self, x):\\n      return json.dumps(x).encode('utf-8')\\n\\n\\\n","          \\    def decode(self, x):\\n      return json.loads(x)\\n\\n  class ChatDatasetProcessor(beam.DoFn):\\n\\\n","          \\    \\\"\\\"\\\"Converts chat data from input format to the format expected by\\\n","          \\ the model.\\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        default_context:\\\n","          \\ str,\\n        prompt_schema: PromptSchema,\\n        dataset_type: str,\\n\\\n","          \\    ):\\n      self._default_context = default_context\\n      self._schema\\\n","          \\ = prompt_schema\\n      self._dataset_type = dataset_type\\n\\n    def _get_messages_or_fail(\\n\\\n","          \\        self, element: Mapping[str, Any]\\n    ) -> List[Mapping[str, str]]:\\n\\\n","          \\      messages = element.get(MESSAGES_KEY)\\n      if not messages:\\n  \\\n","          \\      raise ValueError(\\n            'No messages present. Please include\\\n","          \\ a non-empty '\\n            f'`messages` field in each line of dataset:\\\n","          \\ {element}.'\\n        )\\n      elif messages[0].get(AUTHOR_KEY) != AUTHOR_USER:\\n\\\n","          \\        raise ValueError(f'First author must be the {AUTHOR_USER}: {element}')\\n\\\n","          \\      elif (\\n          self._dataset_type in {PROMPT_DATASET, PREFERENCE_DATASET}\\n\\\n","          \\          and messages[-1].get(AUTHOR_KEY) != AUTHOR_USER\\n      ):\\n \\\n","          \\       raise ValueError(\\n            f'Last author in the {self._dataset_type}\\\n","          \\ dataset must be the'\\n            f' {AUTHOR_USER}: {element}'\\n     \\\n","          \\   )\\n      elif (\\n          self._dataset_type == SUPERVISED_DATASET\\n\\\n","          \\          and messages[-1].get(AUTHOR_KEY) != AUTHOR_ASSISTANT\\n      ):\\n\\\n","          \\        raise ValueError(\\n            f'Last author in the {self._dataset_type}\\\n","          \\ dataset must be the'\\n            f' {AUTHOR_ASSISTANT}: {element}'\\n\\\n","          \\        )\\n      return messages\\n\\n    def _get_or_fail(self, message:\\\n","          \\ Mapping[str, str], key: str) -> str:\\n      value = message.get(key)\\n\\\n","          \\      if not value and value != 0:\\n        raise ValueError(\\n       \\\n","          \\     f'Each message must contain non-empty value for {key}. '\\n       \\\n","          \\     f'Invalid message: {message}'\\n        )\\n      return value\\n\\n \\\n","          \\   def _get_author_or_fail(self, message: Mapping[str, str]) -> str:\\n\\\n","          \\      author = self._get_or_fail(message, AUTHOR_KEY)\\n      if author\\\n","          \\ not in VALID_AUTHORS:\\n        raise ValueError(\\n            'The `author`\\\n","          \\ of each message needs to be from one of'\\n            f' {VALID_AUTHORS}.\\\n","          \\ Got author = {author}.'\\n        )\\n      return author\\n\\n    def process(self,\\\n","          \\ element):\\n      context = element.get(CONTEXT_KEY, self._default_context)\\n\\\n","          \\      messages = self._get_messages_or_fail(element)\\n\\n      message_history\\\n","          \\ = [\\n          self._schema.global_prefix,\\n          self._schema.get_system_message(context),\\n\\\n","          \\      ]\\n      for message in messages:\\n        author = self._get_author_or_fail(message)\\n\\\n","          \\        content = self._get_or_fail(message, CONTENT_KEY)\\n        if author\\\n","          \\ == AUTHOR_USER:\\n          message_history.append(\\n              f'{self._schema.user_prefix}{content}{self._schema.user_postfix}'\\n\\\n","          \\          )\\n        elif author == AUTHOR_ASSISTANT:\\n          message_history.append(self._schema.assistant_prefix)\\n\\\n","          \\          input_text = ''.join(message_history)\\n          # For training\\\n","          \\ datasets yield an example for each user/assistant\\n          # exchange:\\n\\\n","          \\          if self._dataset_type == SUPERVISED_DATASET:\\n            yield\\\n","          \\ {\\n                INPUT_TEXT_KEY: input_text.rstrip(),\\n            \\\n","          \\    OUTPUT_TEXT_KEY: content,\\n            }\\n          message_history\\\n","          \\ = [\\n              input_text,\\n              f'{content}{self._schema.assistant_postfix}',\\n\\\n","          \\          ]\\n        else:\\n          raise ValueError(\\n             \\\n","          \\ f'Unknown author {author}. Must be one of {VALID_AUTHORS}.'\\n        \\\n","          \\  )\\n      # For prompt and preference datasets, only yield an example\\\n","          \\ after the\\n      # final user message:\\n      if self._dataset_type ==\\\n","          \\ PROMPT_DATASET:\\n        message_history.append(self._schema.assistant_prefix)\\n\\\n","          \\        input_text = ''.join(message_history)\\n        yield {INPUT_TEXT_KEY:\\\n","          \\ input_text.rstrip()}\\n      elif self._dataset_type == PREFERENCE_DATASET:\\n\\\n","          \\        message_history.append(self._schema.assistant_prefix)\\n       \\\n","          \\ input_text = ''.join(message_history)\\n        yield {\\n            INPUT_TEXT_KEY:\\\n","          \\ input_text.rstrip(),\\n            CANDIDATE_0_KEY: self._get_or_fail(element,\\\n","          \\ CANDIDATE_0_KEY),\\n            CANDIDATE_1_KEY: self._get_or_fail(element,\\\n","          \\ CANDIDATE_1_KEY),\\n            CHOICE_KEY: self._get_or_fail(element,\\\n","          \\ CHOICE_KEY),\\n        }\\n\\n  # ]\\n\\n  processed_dataset_uri = get_gcs_path(processed_dataset_uri,\\\n","          \\ allow_local_files)\\n\\n  # Reuse the input dataset if no preprocessing\\\n","          \\ is needed.\\n  if large_model_reference.lower() not in MODEL_TO_SCHEMA_MAPPING:\\n\\\n","          \\    with open(processed_dataset_uri, 'w') as f:\\n      f.write(input_dataset_uri)\\n\\\n","          \\    return\\n\\n  prompt_schema = MODEL_TO_SCHEMA_MAPPING[large_model_reference]\\n\\\n","          \\n  # Provide gs:// paths for datasets processed by Beam.\\n  input_dataset_uri\\\n","          \\ = get_gs_path(input_dataset_uri, allow_local_files)\\n  processed_dataset\\\n","          \\ = get_gs_path(processed_dataset, allow_local_files)\\n  os.makedirs(processed_dataset,\\\n","          \\ exist_ok=True)\\n  processed_dataset_prefix = os.path.join(processed_dataset,\\\n","          \\ 'shard')\\n  dataset_type = dataset_type.lower()\\n  if dataset_type not\\\n","          \\ in VALID_DATASETS:\\n    raise ValueError(\\n        f'Unknown dataset type\\\n","          \\ {dataset_type}. Must be one of {VALID_DATASETS}.'\\n    )\\n\\n  pipeline_options\\\n","          \\ = (\\n      beam.options.pipeline_options.PipelineOptions.from_dictionary({\\n\\\n","          \\          'runner': 'DirectRunner',\\n      })\\n  )\\n  with beam.Pipeline(options=pipeline_options)\\\n","          \\ as pipeline:\\n    _ = (\\n        pipeline\\n        | 'Read JSON from input\\\n","          \\ dataset'\\n        >> beam.io.ReadFromText(input_dataset_uri, coder=JsonCoder())\\n\\\n","          \\        | 'Process chat dataset'\\n        >> beam.ParDo(\\n            ChatDatasetProcessor(\\n\\\n","          \\                default_context=default_context,\\n                prompt_schema=prompt_schema,\\n\\\n","          \\                dataset_type=dataset_type,\\n            )\\n        )\\n\\\n","          \\        | 'Write processed JSON to output file'\\n        >> beam.io.WriteToText(\\n\\\n","          \\            file_path_prefix=processed_dataset_prefix,\\n            file_name_suffix='.jsonl',\\n\\\n","          \\            coder=JsonCoder(),\\n        )\\n    )\\n\\n  # Write file pattern\\\n","          \\ that the tokenizer can use to find all processed files.\\n  with open(processed_dataset_uri,\\\n","          \\ 'w') as f:\\n    processed_dataset_pattern = os.path.join(processed_dataset,\\\n","          \\ '*.jsonl')\\n    f.write(processed_dataset_pattern)\\n\\n\"\n","        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.16.1\n","    exec-preprocess-chat-dataset-2:\n","      container:\n","        args:\n","        - --executor_input\n","        - '{{$}}'\n","        - --function_to_execute\n","        - preprocess_chat_dataset\n","        command:\n","        - sh\n","        - -ec\n","        - 'program_path=$(mktemp -d)\n","\n","\n","          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n","\n","          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n","\n","          '\n","        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\n","          \\ *\\n\\ndef preprocess_chat_dataset(\\n    large_model_reference: str,\\n \\\n","          \\   input_dataset_uri: str,\\n    dataset_type: str,\\n    processed_dataset:\\\n","          \\ dsl.OutputPath(dsl.Artifact),  # pytype: disable=invalid-annotation\\n\\\n","          \\    processed_dataset_uri: dsl.OutputPath(str),  # pytype: disable=invalid-annotation\\n\\\n","          \\    default_context: str = '',\\n    allow_local_files: bool = False,\\n\\\n","          ):  # pylint: disable=g-doc-args\\n  # fmt: off\\n  \\\"\\\"\\\"Preprocesses datasets\\\n","          \\ before tokenization.\\n\\n  For text datasets, this is a no-op.\\n\\n  Args:\\n\\\n","          \\    large_model_reference: Name of the base model. Supported values are\\\n","          \\ `text-bison@001`, `chat-bison@001`, `t5-small`, `t5-large`, `t5-xl` and\\\n","          \\ `t5-xxl`. `text-bison@001`, `chat-bison@001` and `t5-small` are supported\\\n","          \\ in ``us-central1` and `europe-west4`. `t5-large`, `t5-xl` and `t5-xxl`\\\n","          \\ are only supported in `europe-west4`.\\n    input_dataset_uri: Path to\\\n","          \\ an unprocessed JSONL dataset.\\n    default_context: Default context to\\\n","          \\ apply to each example if a chat model is specified.\\n    allow_local_files:\\\n","          \\ Whether input URIs can specify local file paths.\\n    is_prompt_dataset:\\\n","          \\ Whether the input dataset contains prompts for inference. In this case,\\\n","          \\ the last author in `messages` should be the `user`, and the output dataset\\\n","          \\ will only contain `input_text`.\\n\\n  Returns:\\n    processed_dataset:\\\n","          \\ Processed chat dataset. Each example will contain fields `input_text`,\\\n","          \\ and if the input dataset is not a prompt dataset example will also contain\\\n","          \\ `output_text`.\\n    processed_dataset_uri: String pattern that can be\\\n","          \\ used to find the processed dataset in downstream components.\\n\\n  \\\"\\\"\\\n","          \\\"\\n  # fmt: on\\n  # pylint: disable=g-import-not-at-top\\n  import dataclasses\\n\\\n","          \\  import json\\n  import os\\n  from typing import Any, Callable, List, Mapping\\n\\\n","          \\  import apache_beam as beam\\n  # pylint: enable=g-import-not-at-top\\n\\n\\\n","          \\  # [ Define helper methods and classes for preprocessing\\n  # pylint:\\\n","          \\ disable=invalid-name\\n  INPUT_TEXT_KEY = 'input_text'\\n  OUTPUT_TEXT_KEY\\\n","          \\ = 'output_text'\\n  CONTEXT_KEY = 'context'\\n  MESSAGES_KEY = 'messages'\\n\\\n","          \\  CANDIDATE_0_KEY = 'candidate_0'\\n  CANDIDATE_1_KEY = 'candidate_1'\\n\\\n","          \\  CHOICE_KEY = 'choice'\\n  AUTHOR_KEY = 'author'\\n  CONTENT_KEY = 'content'\\n\\\n","          \\  AUTHOR_USER = 'user'\\n  AUTHOR_ASSISTANT = 'assistant'\\n  VALID_AUTHORS\\\n","          \\ = {AUTHOR_USER, AUTHOR_ASSISTANT}\\n  SUPERVISED_DATASET = 'supervised'\\n\\\n","          \\  PROMPT_DATASET = 'prompt'\\n  PREFERENCE_DATASET = 'preference'\\n  VALID_DATASETS\\\n","          \\ = {SUPERVISED_DATASET, PROMPT_DATASET, PREFERENCE_DATASET}\\n  # pylint:\\\n","          \\ enable=invalid-name\\n\\n  @dataclasses.dataclass\\n  class PromptSchema:\\n\\\n","          \\    global_prefix: str\\n    user_prefix: str\\n    user_postfix: str\\n \\\n","          \\   assistant_prefix: str\\n    assistant_postfix: str\\n    get_system_message:\\\n","          \\ Callable[[str], str]  # pytype: disable=invalid-annotation\\n\\n  def _get_chat_bison_001_system_message(context:\\\n","          \\ str) -> str:\\n    return f'[SYSTEM]:{context}\\\\n\\\\n' if context else ''\\n\\\n","          \\n  chat_bison_001_schema = PromptSchema(\\n      global_prefix=(\\n     \\\n","          \\     'Only answer after [assistant] and never reply as [user]:\\\\n'\\n  \\\n","          \\    ),\\n      get_system_message=_get_chat_bison_001_system_message,\\n\\\n","          \\      user_prefix='[user]:',\\n      user_postfix='\\\\n',\\n      assistant_prefix='[assistant]:',\\n\\\n","          \\      assistant_postfix='\\\\n',\\n  )\\n\\n  def _get_chat_llama_system_message(context:\\\n","          \\ str) -> str:\\n    return f'<<SYS>>\\\\n{context}\\\\n<</SYS>>\\\\n\\\\n' if context\\\n","          \\ else ''\\n\\n  chat_llama_schema = PromptSchema(\\n      global_prefix='<s>[INST]\\\n","          \\ ',\\n      get_system_message=_get_chat_llama_system_message,\\n      user_prefix='',\\n\\\n","          \\      user_postfix=' [/INST]',\\n      assistant_prefix=' ',\\n      assistant_postfix='</s><s>[INST]\\\n","          \\ ',\\n  )\\n\\n  MODEL_TO_SCHEMA_MAPPING = {  # pylint: disable=invalid-name\\n\\\n","          \\      'chat-bison@001': chat_bison_001_schema,\\n      'llama-2-7b-chat':\\\n","          \\ chat_llama_schema,\\n      'llama-2-13b-chat': chat_llama_schema,\\n  }\\n\\\n","          \\n  def get_gcs_path(input_path: str, allow_local_files: bool) -> str:\\n\\\n","          \\    \\\"\\\"\\\"Gets the /gcs/ path for a given URI.\\\"\\\"\\\"\\n    if input_path.startswith('gs://'):\\n\\\n","          \\      return input_path.replace('gs://', '/gcs/', 1)\\n    elif input_path.startswith('/gcs/')\\\n","          \\ or allow_local_files:\\n      return input_path\\n    else:\\n      raise\\\n","          \\ ValueError(\\n          f'Invalid Cloud storage URI {input_path}. '\\n \\\n","          \\         'Must start with `gs://` or `/gcs/`.'\\n      )\\n\\n  def get_gs_path(input_path:\\\n","          \\ str, allow_local_files: bool) -> str:\\n    \\\"\\\"\\\"Gets the gs:// path for\\\n","          \\ a given URI.\\\"\\\"\\\"\\n    if input_path.startswith('/gcs/'):\\n      return\\\n","          \\ input_path.replace('/gcs/', 'gs://', 1)\\n    elif input_path.startswith('gs://')\\\n","          \\ or allow_local_files:\\n      return input_path\\n    else:\\n      raise\\\n","          \\ ValueError(\\n          f'Invalid Cloud storage URI {input_path}. '\\n \\\n","          \\         'Must start with `gs://` or `/gcs/`.'\\n      )\\n\\n  class JsonCoder(beam.coders.Coder):\\n\\\n","          \\    \\\"\\\"\\\"A coder that encodes/decodes lines as JSON strings.\\\"\\\"\\\"\\n\\n\\\n","          \\    def encode(self, x):\\n      return json.dumps(x).encode('utf-8')\\n\\n\\\n","          \\    def decode(self, x):\\n      return json.loads(x)\\n\\n  class ChatDatasetProcessor(beam.DoFn):\\n\\\n","          \\    \\\"\\\"\\\"Converts chat data from input format to the format expected by\\\n","          \\ the model.\\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        default_context:\\\n","          \\ str,\\n        prompt_schema: PromptSchema,\\n        dataset_type: str,\\n\\\n","          \\    ):\\n      self._default_context = default_context\\n      self._schema\\\n","          \\ = prompt_schema\\n      self._dataset_type = dataset_type\\n\\n    def _get_messages_or_fail(\\n\\\n","          \\        self, element: Mapping[str, Any]\\n    ) -> List[Mapping[str, str]]:\\n\\\n","          \\      messages = element.get(MESSAGES_KEY)\\n      if not messages:\\n  \\\n","          \\      raise ValueError(\\n            'No messages present. Please include\\\n","          \\ a non-empty '\\n            f'`messages` field in each line of dataset:\\\n","          \\ {element}.'\\n        )\\n      elif messages[0].get(AUTHOR_KEY) != AUTHOR_USER:\\n\\\n","          \\        raise ValueError(f'First author must be the {AUTHOR_USER}: {element}')\\n\\\n","          \\      elif (\\n          self._dataset_type in {PROMPT_DATASET, PREFERENCE_DATASET}\\n\\\n","          \\          and messages[-1].get(AUTHOR_KEY) != AUTHOR_USER\\n      ):\\n \\\n","          \\       raise ValueError(\\n            f'Last author in the {self._dataset_type}\\\n","          \\ dataset must be the'\\n            f' {AUTHOR_USER}: {element}'\\n     \\\n","          \\   )\\n      elif (\\n          self._dataset_type == SUPERVISED_DATASET\\n\\\n","          \\          and messages[-1].get(AUTHOR_KEY) != AUTHOR_ASSISTANT\\n      ):\\n\\\n","          \\        raise ValueError(\\n            f'Last author in the {self._dataset_type}\\\n","          \\ dataset must be the'\\n            f' {AUTHOR_ASSISTANT}: {element}'\\n\\\n","          \\        )\\n      return messages\\n\\n    def _get_or_fail(self, message:\\\n","          \\ Mapping[str, str], key: str) -> str:\\n      value = message.get(key)\\n\\\n","          \\      if not value and value != 0:\\n        raise ValueError(\\n       \\\n","          \\     f'Each message must contain non-empty value for {key}. '\\n       \\\n","          \\     f'Invalid message: {message}'\\n        )\\n      return value\\n\\n \\\n","          \\   def _get_author_or_fail(self, message: Mapping[str, str]) -> str:\\n\\\n","          \\      author = self._get_or_fail(message, AUTHOR_KEY)\\n      if author\\\n","          \\ not in VALID_AUTHORS:\\n        raise ValueError(\\n            'The `author`\\\n","          \\ of each message needs to be from one of'\\n            f' {VALID_AUTHORS}.\\\n","          \\ Got author = {author}.'\\n        )\\n      return author\\n\\n    def process(self,\\\n","          \\ element):\\n      context = element.get(CONTEXT_KEY, self._default_context)\\n\\\n","          \\      messages = self._get_messages_or_fail(element)\\n\\n      message_history\\\n","          \\ = [\\n          self._schema.global_prefix,\\n          self._schema.get_system_message(context),\\n\\\n","          \\      ]\\n      for message in messages:\\n        author = self._get_author_or_fail(message)\\n\\\n","          \\        content = self._get_or_fail(message, CONTENT_KEY)\\n        if author\\\n","          \\ == AUTHOR_USER:\\n          message_history.append(\\n              f'{self._schema.user_prefix}{content}{self._schema.user_postfix}'\\n\\\n","          \\          )\\n        elif author == AUTHOR_ASSISTANT:\\n          message_history.append(self._schema.assistant_prefix)\\n\\\n","          \\          input_text = ''.join(message_history)\\n          # For training\\\n","          \\ datasets yield an example for each user/assistant\\n          # exchange:\\n\\\n","          \\          if self._dataset_type == SUPERVISED_DATASET:\\n            yield\\\n","          \\ {\\n                INPUT_TEXT_KEY: input_text.rstrip(),\\n            \\\n","          \\    OUTPUT_TEXT_KEY: content,\\n            }\\n          message_history\\\n","          \\ = [\\n              input_text,\\n              f'{content}{self._schema.assistant_postfix}',\\n\\\n","          \\          ]\\n        else:\\n          raise ValueError(\\n             \\\n","          \\ f'Unknown author {author}. Must be one of {VALID_AUTHORS}.'\\n        \\\n","          \\  )\\n      # For prompt and preference datasets, only yield an example\\\n","          \\ after the\\n      # final user message:\\n      if self._dataset_type ==\\\n","          \\ PROMPT_DATASET:\\n        message_history.append(self._schema.assistant_prefix)\\n\\\n","          \\        input_text = ''.join(message_history)\\n        yield {INPUT_TEXT_KEY:\\\n","          \\ input_text.rstrip()}\\n      elif self._dataset_type == PREFERENCE_DATASET:\\n\\\n","          \\        message_history.append(self._schema.assistant_prefix)\\n       \\\n","          \\ input_text = ''.join(message_history)\\n        yield {\\n            INPUT_TEXT_KEY:\\\n","          \\ input_text.rstrip(),\\n            CANDIDATE_0_KEY: self._get_or_fail(element,\\\n","          \\ CANDIDATE_0_KEY),\\n            CANDIDATE_1_KEY: self._get_or_fail(element,\\\n","          \\ CANDIDATE_1_KEY),\\n            CHOICE_KEY: self._get_or_fail(element,\\\n","          \\ CHOICE_KEY),\\n        }\\n\\n  # ]\\n\\n  processed_dataset_uri = get_gcs_path(processed_dataset_uri,\\\n","          \\ allow_local_files)\\n\\n  # Reuse the input dataset if no preprocessing\\\n","          \\ is needed.\\n  if large_model_reference.lower() not in MODEL_TO_SCHEMA_MAPPING:\\n\\\n","          \\    with open(processed_dataset_uri, 'w') as f:\\n      f.write(input_dataset_uri)\\n\\\n","          \\    return\\n\\n  prompt_schema = MODEL_TO_SCHEMA_MAPPING[large_model_reference]\\n\\\n","          \\n  # Provide gs:// paths for datasets processed by Beam.\\n  input_dataset_uri\\\n","          \\ = get_gs_path(input_dataset_uri, allow_local_files)\\n  processed_dataset\\\n","          \\ = get_gs_path(processed_dataset, allow_local_files)\\n  os.makedirs(processed_dataset,\\\n","          \\ exist_ok=True)\\n  processed_dataset_prefix = os.path.join(processed_dataset,\\\n","          \\ 'shard')\\n  dataset_type = dataset_type.lower()\\n  if dataset_type not\\\n","          \\ in VALID_DATASETS:\\n    raise ValueError(\\n        f'Unknown dataset type\\\n","          \\ {dataset_type}. Must be one of {VALID_DATASETS}.'\\n    )\\n\\n  pipeline_options\\\n","          \\ = (\\n      beam.options.pipeline_options.PipelineOptions.from_dictionary({\\n\\\n","          \\          'runner': 'DirectRunner',\\n      })\\n  )\\n  with beam.Pipeline(options=pipeline_options)\\\n","          \\ as pipeline:\\n    _ = (\\n        pipeline\\n        | 'Read JSON from input\\\n","          \\ dataset'\\n        >> beam.io.ReadFromText(input_dataset_uri, coder=JsonCoder())\\n\\\n","          \\        | 'Process chat dataset'\\n        >> beam.ParDo(\\n            ChatDatasetProcessor(\\n\\\n","          \\                default_context=default_context,\\n                prompt_schema=prompt_schema,\\n\\\n","          \\                dataset_type=dataset_type,\\n            )\\n        )\\n\\\n","          \\        | 'Write processed JSON to output file'\\n        >> beam.io.WriteToText(\\n\\\n","          \\            file_path_prefix=processed_dataset_prefix,\\n            file_name_suffix='.jsonl',\\n\\\n","          \\            coder=JsonCoder(),\\n        )\\n    )\\n\\n  # Write file pattern\\\n","          \\ that the tokenizer can use to find all processed files.\\n  with open(processed_dataset_uri,\\\n","          \\ 'w') as f:\\n    processed_dataset_pattern = os.path.join(processed_dataset,\\\n","          \\ '*.jsonl')\\n    f.write(processed_dataset_pattern)\\n\\n\"\n","        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.16.1\n","    exec-preprocess-chat-dataset-3:\n","      container:\n","        args:\n","        - --executor_input\n","        - '{{$}}'\n","        - --function_to_execute\n","        - preprocess_chat_dataset\n","        command:\n","        - sh\n","        - -ec\n","        - 'program_path=$(mktemp -d)\n","\n","\n","          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n","\n","          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n","\n","          '\n","        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\n","          \\ *\\n\\ndef preprocess_chat_dataset(\\n    large_model_reference: str,\\n \\\n","          \\   input_dataset_uri: str,\\n    dataset_type: str,\\n    processed_dataset:\\\n","          \\ dsl.OutputPath(dsl.Artifact),  # pytype: disable=invalid-annotation\\n\\\n","          \\    processed_dataset_uri: dsl.OutputPath(str),  # pytype: disable=invalid-annotation\\n\\\n","          \\    default_context: str = '',\\n    allow_local_files: bool = False,\\n\\\n","          ):  # pylint: disable=g-doc-args\\n  # fmt: off\\n  \\\"\\\"\\\"Preprocesses datasets\\\n","          \\ before tokenization.\\n\\n  For text datasets, this is a no-op.\\n\\n  Args:\\n\\\n","          \\    large_model_reference: Name of the base model. Supported values are\\\n","          \\ `text-bison@001`, `chat-bison@001`, `t5-small`, `t5-large`, `t5-xl` and\\\n","          \\ `t5-xxl`. `text-bison@001`, `chat-bison@001` and `t5-small` are supported\\\n","          \\ in ``us-central1` and `europe-west4`. `t5-large`, `t5-xl` and `t5-xxl`\\\n","          \\ are only supported in `europe-west4`.\\n    input_dataset_uri: Path to\\\n","          \\ an unprocessed JSONL dataset.\\n    default_context: Default context to\\\n","          \\ apply to each example if a chat model is specified.\\n    allow_local_files:\\\n","          \\ Whether input URIs can specify local file paths.\\n    is_prompt_dataset:\\\n","          \\ Whether the input dataset contains prompts for inference. In this case,\\\n","          \\ the last author in `messages` should be the `user`, and the output dataset\\\n","          \\ will only contain `input_text`.\\n\\n  Returns:\\n    processed_dataset:\\\n","          \\ Processed chat dataset. Each example will contain fields `input_text`,\\\n","          \\ and if the input dataset is not a prompt dataset example will also contain\\\n","          \\ `output_text`.\\n    processed_dataset_uri: String pattern that can be\\\n","          \\ used to find the processed dataset in downstream components.\\n\\n  \\\"\\\"\\\n","          \\\"\\n  # fmt: on\\n  # pylint: disable=g-import-not-at-top\\n  import dataclasses\\n\\\n","          \\  import json\\n  import os\\n  from typing import Any, Callable, List, Mapping\\n\\\n","          \\  import apache_beam as beam\\n  # pylint: enable=g-import-not-at-top\\n\\n\\\n","          \\  # [ Define helper methods and classes for preprocessing\\n  # pylint:\\\n","          \\ disable=invalid-name\\n  INPUT_TEXT_KEY = 'input_text'\\n  OUTPUT_TEXT_KEY\\\n","          \\ = 'output_text'\\n  CONTEXT_KEY = 'context'\\n  MESSAGES_KEY = 'messages'\\n\\\n","          \\  CANDIDATE_0_KEY = 'candidate_0'\\n  CANDIDATE_1_KEY = 'candidate_1'\\n\\\n","          \\  CHOICE_KEY = 'choice'\\n  AUTHOR_KEY = 'author'\\n  CONTENT_KEY = 'content'\\n\\\n","          \\  AUTHOR_USER = 'user'\\n  AUTHOR_ASSISTANT = 'assistant'\\n  VALID_AUTHORS\\\n","          \\ = {AUTHOR_USER, AUTHOR_ASSISTANT}\\n  SUPERVISED_DATASET = 'supervised'\\n\\\n","          \\  PROMPT_DATASET = 'prompt'\\n  PREFERENCE_DATASET = 'preference'\\n  VALID_DATASETS\\\n","          \\ = {SUPERVISED_DATASET, PROMPT_DATASET, PREFERENCE_DATASET}\\n  # pylint:\\\n","          \\ enable=invalid-name\\n\\n  @dataclasses.dataclass\\n  class PromptSchema:\\n\\\n","          \\    global_prefix: str\\n    user_prefix: str\\n    user_postfix: str\\n \\\n","          \\   assistant_prefix: str\\n    assistant_postfix: str\\n    get_system_message:\\\n","          \\ Callable[[str], str]  # pytype: disable=invalid-annotation\\n\\n  def _get_chat_bison_001_system_message(context:\\\n","          \\ str) -> str:\\n    return f'[SYSTEM]:{context}\\\\n\\\\n' if context else ''\\n\\\n","          \\n  chat_bison_001_schema = PromptSchema(\\n      global_prefix=(\\n     \\\n","          \\     'Only answer after [assistant] and never reply as [user]:\\\\n'\\n  \\\n","          \\    ),\\n      get_system_message=_get_chat_bison_001_system_message,\\n\\\n","          \\      user_prefix='[user]:',\\n      user_postfix='\\\\n',\\n      assistant_prefix='[assistant]:',\\n\\\n","          \\      assistant_postfix='\\\\n',\\n  )\\n\\n  def _get_chat_llama_system_message(context:\\\n","          \\ str) -> str:\\n    return f'<<SYS>>\\\\n{context}\\\\n<</SYS>>\\\\n\\\\n' if context\\\n","          \\ else ''\\n\\n  chat_llama_schema = PromptSchema(\\n      global_prefix='<s>[INST]\\\n","          \\ ',\\n      get_system_message=_get_chat_llama_system_message,\\n      user_prefix='',\\n\\\n","          \\      user_postfix=' [/INST]',\\n      assistant_prefix=' ',\\n      assistant_postfix='</s><s>[INST]\\\n","          \\ ',\\n  )\\n\\n  MODEL_TO_SCHEMA_MAPPING = {  # pylint: disable=invalid-name\\n\\\n","          \\      'chat-bison@001': chat_bison_001_schema,\\n      'llama-2-7b-chat':\\\n","          \\ chat_llama_schema,\\n      'llama-2-13b-chat': chat_llama_schema,\\n  }\\n\\\n","          \\n  def get_gcs_path(input_path: str, allow_local_files: bool) -> str:\\n\\\n","          \\    \\\"\\\"\\\"Gets the /gcs/ path for a given URI.\\\"\\\"\\\"\\n    if input_path.startswith('gs://'):\\n\\\n","          \\      return input_path.replace('gs://', '/gcs/', 1)\\n    elif input_path.startswith('/gcs/')\\\n","          \\ or allow_local_files:\\n      return input_path\\n    else:\\n      raise\\\n","          \\ ValueError(\\n          f'Invalid Cloud storage URI {input_path}. '\\n \\\n","          \\         'Must start with `gs://` or `/gcs/`.'\\n      )\\n\\n  def get_gs_path(input_path:\\\n","          \\ str, allow_local_files: bool) -> str:\\n    \\\"\\\"\\\"Gets the gs:// path for\\\n","          \\ a given URI.\\\"\\\"\\\"\\n    if input_path.startswith('/gcs/'):\\n      return\\\n","          \\ input_path.replace('/gcs/', 'gs://', 1)\\n    elif input_path.startswith('gs://')\\\n","          \\ or allow_local_files:\\n      return input_path\\n    else:\\n      raise\\\n","          \\ ValueError(\\n          f'Invalid Cloud storage URI {input_path}. '\\n \\\n","          \\         'Must start with `gs://` or `/gcs/`.'\\n      )\\n\\n  class JsonCoder(beam.coders.Coder):\\n\\\n","          \\    \\\"\\\"\\\"A coder that encodes/decodes lines as JSON strings.\\\"\\\"\\\"\\n\\n\\\n","          \\    def encode(self, x):\\n      return json.dumps(x).encode('utf-8')\\n\\n\\\n","          \\    def decode(self, x):\\n      return json.loads(x)\\n\\n  class ChatDatasetProcessor(beam.DoFn):\\n\\\n","          \\    \\\"\\\"\\\"Converts chat data from input format to the format expected by\\\n","          \\ the model.\\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        default_context:\\\n","          \\ str,\\n        prompt_schema: PromptSchema,\\n        dataset_type: str,\\n\\\n","          \\    ):\\n      self._default_context = default_context\\n      self._schema\\\n","          \\ = prompt_schema\\n      self._dataset_type = dataset_type\\n\\n    def _get_messages_or_fail(\\n\\\n","          \\        self, element: Mapping[str, Any]\\n    ) -> List[Mapping[str, str]]:\\n\\\n","          \\      messages = element.get(MESSAGES_KEY)\\n      if not messages:\\n  \\\n","          \\      raise ValueError(\\n            'No messages present. Please include\\\n","          \\ a non-empty '\\n            f'`messages` field in each line of dataset:\\\n","          \\ {element}.'\\n        )\\n      elif messages[0].get(AUTHOR_KEY) != AUTHOR_USER:\\n\\\n","          \\        raise ValueError(f'First author must be the {AUTHOR_USER}: {element}')\\n\\\n","          \\      elif (\\n          self._dataset_type in {PROMPT_DATASET, PREFERENCE_DATASET}\\n\\\n","          \\          and messages[-1].get(AUTHOR_KEY) != AUTHOR_USER\\n      ):\\n \\\n","          \\       raise ValueError(\\n            f'Last author in the {self._dataset_type}\\\n","          \\ dataset must be the'\\n            f' {AUTHOR_USER}: {element}'\\n     \\\n","          \\   )\\n      elif (\\n          self._dataset_type == SUPERVISED_DATASET\\n\\\n","          \\          and messages[-1].get(AUTHOR_KEY) != AUTHOR_ASSISTANT\\n      ):\\n\\\n","          \\        raise ValueError(\\n            f'Last author in the {self._dataset_type}\\\n","          \\ dataset must be the'\\n            f' {AUTHOR_ASSISTANT}: {element}'\\n\\\n","          \\        )\\n      return messages\\n\\n    def _get_or_fail(self, message:\\\n","          \\ Mapping[str, str], key: str) -> str:\\n      value = message.get(key)\\n\\\n","          \\      if not value and value != 0:\\n        raise ValueError(\\n       \\\n","          \\     f'Each message must contain non-empty value for {key}. '\\n       \\\n","          \\     f'Invalid message: {message}'\\n        )\\n      return value\\n\\n \\\n","          \\   def _get_author_or_fail(self, message: Mapping[str, str]) -> str:\\n\\\n","          \\      author = self._get_or_fail(message, AUTHOR_KEY)\\n      if author\\\n","          \\ not in VALID_AUTHORS:\\n        raise ValueError(\\n            'The `author`\\\n","          \\ of each message needs to be from one of'\\n            f' {VALID_AUTHORS}.\\\n","          \\ Got author = {author}.'\\n        )\\n      return author\\n\\n    def process(self,\\\n","          \\ element):\\n      context = element.get(CONTEXT_KEY, self._default_context)\\n\\\n","          \\      messages = self._get_messages_or_fail(element)\\n\\n      message_history\\\n","          \\ = [\\n          self._schema.global_prefix,\\n          self._schema.get_system_message(context),\\n\\\n","          \\      ]\\n      for message in messages:\\n        author = self._get_author_or_fail(message)\\n\\\n","          \\        content = self._get_or_fail(message, CONTENT_KEY)\\n        if author\\\n","          \\ == AUTHOR_USER:\\n          message_history.append(\\n              f'{self._schema.user_prefix}{content}{self._schema.user_postfix}'\\n\\\n","          \\          )\\n        elif author == AUTHOR_ASSISTANT:\\n          message_history.append(self._schema.assistant_prefix)\\n\\\n","          \\          input_text = ''.join(message_history)\\n          # For training\\\n","          \\ datasets yield an example for each user/assistant\\n          # exchange:\\n\\\n","          \\          if self._dataset_type == SUPERVISED_DATASET:\\n            yield\\\n","          \\ {\\n                INPUT_TEXT_KEY: input_text.rstrip(),\\n            \\\n","          \\    OUTPUT_TEXT_KEY: content,\\n            }\\n          message_history\\\n","          \\ = [\\n              input_text,\\n              f'{content}{self._schema.assistant_postfix}',\\n\\\n","          \\          ]\\n        else:\\n          raise ValueError(\\n             \\\n","          \\ f'Unknown author {author}. Must be one of {VALID_AUTHORS}.'\\n        \\\n","          \\  )\\n      # For prompt and preference datasets, only yield an example\\\n","          \\ after the\\n      # final user message:\\n      if self._dataset_type ==\\\n","          \\ PROMPT_DATASET:\\n        message_history.append(self._schema.assistant_prefix)\\n\\\n","          \\        input_text = ''.join(message_history)\\n        yield {INPUT_TEXT_KEY:\\\n","          \\ input_text.rstrip()}\\n      elif self._dataset_type == PREFERENCE_DATASET:\\n\\\n","          \\        message_history.append(self._schema.assistant_prefix)\\n       \\\n","          \\ input_text = ''.join(message_history)\\n        yield {\\n            INPUT_TEXT_KEY:\\\n","          \\ input_text.rstrip(),\\n            CANDIDATE_0_KEY: self._get_or_fail(element,\\\n","          \\ CANDIDATE_0_KEY),\\n            CANDIDATE_1_KEY: self._get_or_fail(element,\\\n","          \\ CANDIDATE_1_KEY),\\n            CHOICE_KEY: self._get_or_fail(element,\\\n","          \\ CHOICE_KEY),\\n        }\\n\\n  # ]\\n\\n  processed_dataset_uri = get_gcs_path(processed_dataset_uri,\\\n","          \\ allow_local_files)\\n\\n  # Reuse the input dataset if no preprocessing\\\n","          \\ is needed.\\n  if large_model_reference.lower() not in MODEL_TO_SCHEMA_MAPPING:\\n\\\n","          \\    with open(processed_dataset_uri, 'w') as f:\\n      f.write(input_dataset_uri)\\n\\\n","          \\    return\\n\\n  prompt_schema = MODEL_TO_SCHEMA_MAPPING[large_model_reference]\\n\\\n","          \\n  # Provide gs:// paths for datasets processed by Beam.\\n  input_dataset_uri\\\n","          \\ = get_gs_path(input_dataset_uri, allow_local_files)\\n  processed_dataset\\\n","          \\ = get_gs_path(processed_dataset, allow_local_files)\\n  os.makedirs(processed_dataset,\\\n","          \\ exist_ok=True)\\n  processed_dataset_prefix = os.path.join(processed_dataset,\\\n","          \\ 'shard')\\n  dataset_type = dataset_type.lower()\\n  if dataset_type not\\\n","          \\ in VALID_DATASETS:\\n    raise ValueError(\\n        f'Unknown dataset type\\\n","          \\ {dataset_type}. Must be one of {VALID_DATASETS}.'\\n    )\\n\\n  pipeline_options\\\n","          \\ = (\\n      beam.options.pipeline_options.PipelineOptions.from_dictionary({\\n\\\n","          \\          'runner': 'DirectRunner',\\n      })\\n  )\\n  with beam.Pipeline(options=pipeline_options)\\\n","          \\ as pipeline:\\n    _ = (\\n        pipeline\\n        | 'Read JSON from input\\\n","          \\ dataset'\\n        >> beam.io.ReadFromText(input_dataset_uri, coder=JsonCoder())\\n\\\n","          \\        | 'Process chat dataset'\\n        >> beam.ParDo(\\n            ChatDatasetProcessor(\\n\\\n","          \\                default_context=default_context,\\n                prompt_schema=prompt_schema,\\n\\\n","          \\                dataset_type=dataset_type,\\n            )\\n        )\\n\\\n","          \\        | 'Write processed JSON to output file'\\n        >> beam.io.WriteToText(\\n\\\n","          \\            file_path_prefix=processed_dataset_prefix,\\n            file_name_suffix='.jsonl',\\n\\\n","          \\            coder=JsonCoder(),\\n        )\\n    )\\n\\n  # Write file pattern\\\n","          \\ that the tokenizer can use to find all processed files.\\n  with open(processed_dataset_uri,\\\n","          \\ 'w') as f:\\n    processed_dataset_pattern = os.path.join(processed_dataset,\\\n","          \\ '*.jsonl')\\n    f.write(processed_dataset_pattern)\\n\\n\"\n","        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.16.1\n","    exec-private-text-comparison-importer:\n","      container:\n","        args:\n","        - --type\n","        - CustomJob\n","        - --payload\n","        - '{\"display_name\": \"TfdsComparisonImporter\", \"job_spec\": {\"worker_pool_specs\":\n","          [{\"replica_count\": \"1\", \"machine_spec\": {\"machine_type\": \"{{$.inputs.parameters[''machine_type'']}}\"},\n","          \"container_spec\": {\"image_uri\": \"{{$.inputs.parameters[''image_uri'']}}\",\n","          \"args\": [\"--app_name=text_comparison_importer\", \"--input_text={{$.inputs.parameters[''input_text'']}}\",\n","          \"--inputs_field_name={{$.inputs.parameters[''inputs_field_name'']}}\", \"--comma_separated_candidates_field_names={{$.inputs.parameters[''comma_separated_candidates_field_names'']}}\",\n","          \"--choice_field_name={{$.inputs.parameters[''choice_field_name'']}}\", \"--split={{$.inputs.parameters[''split'']}}\",\n","          \"--output_cache_dir={{$.outputs.parameters[''output_dataset_path''].output_file}}\",\n","          \"--instruction={{$.inputs.parameters[''instruction'']}}\", \"--large_model_reference={{$.inputs.parameters[''large_model_reference'']}}\",\n","          \"--private_bucket_subdir={{$.pipeline_task_name}}_{{$.pipeline_task_uuid}}\"]}}]},\n","          \"encryption_spec\": {\"kms_key_name\": \"{{$.inputs.parameters[''encryption_spec_key_name'']}}\"}}'\n","        - --project\n","        - '{{$.inputs.parameters[''project'']}}'\n","        - --location\n","        - '{{$.inputs.parameters[''location'']}}'\n","        - --gcp_resources\n","        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'\n","        command:\n","        - python3\n","        - -u\n","        - -m\n","        - google_cloud_pipeline_components.container.v1.custom_job.launcher\n","        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.16.1\n","    exec-private-text-comparison-importer-2:\n","      container:\n","        args:\n","        - --type\n","        - CustomJob\n","        - --payload\n","        - '{\"display_name\": \"TfdsComparisonImporter\", \"job_spec\": {\"worker_pool_specs\":\n","          [{\"replica_count\": \"1\", \"machine_spec\": {\"machine_type\": \"{{$.inputs.parameters[''machine_type'']}}\"},\n","          \"container_spec\": {\"image_uri\": \"{{$.inputs.parameters[''image_uri'']}}\",\n","          \"args\": [\"--app_name=text_comparison_importer\", \"--input_text={{$.inputs.parameters[''input_text'']}}\",\n","          \"--inputs_field_name={{$.inputs.parameters[''inputs_field_name'']}}\", \"--comma_separated_candidates_field_names={{$.inputs.parameters[''comma_separated_candidates_field_names'']}}\",\n","          \"--choice_field_name={{$.inputs.parameters[''choice_field_name'']}}\", \"--split={{$.inputs.parameters[''split'']}}\",\n","          \"--output_cache_dir={{$.outputs.parameters[''output_dataset_path''].output_file}}\",\n","          \"--instruction={{$.inputs.parameters[''instruction'']}}\", \"--large_model_reference={{$.inputs.parameters[''large_model_reference'']}}\",\n","          \"--private_bucket_subdir={{$.pipeline_task_name}}_{{$.pipeline_task_uuid}}\"]}}]},\n","          \"encryption_spec\": {\"kms_key_name\": \"{{$.inputs.parameters[''encryption_spec_key_name'']}}\"}}'\n","        - --project\n","        - '{{$.inputs.parameters[''project'']}}'\n","        - --location\n","        - '{{$.inputs.parameters[''location'']}}'\n","        - --gcp_resources\n","        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'\n","        command:\n","        - python3\n","        - -u\n","        - -m\n","        - google_cloud_pipeline_components.container.v1.custom_job.launcher\n","        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.16.1\n","    exec-private-text-importer:\n","      container:\n","        args:\n","        - --type\n","        - CustomJob\n","        - --payload\n","        - '{\"display_name\": \"TextImporter\", \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\":\n","          \"1\", \"machine_spec\": {\"machine_type\": \"{{$.inputs.parameters[''machine_type'']}}\"},\n","          \"container_spec\": {\"image_uri\": \"{{$.inputs.parameters[''image_uri'']}}\",\n","          \"args\": [\"--app_name=text_importer\", \"--input_text={{$.inputs.parameters[''input_text'']}}\",\n","          \"--inputs_field_name={{$.inputs.parameters[''inputs_field_name'']}}\", \"--targets_field_name={{$.inputs.parameters[''targets_field_name'']}}\",\n","          \"--output_split_name={{$.inputs.parameters[''output_split_name'']}}\", \"--instruction={{$.inputs.parameters[''instruction'']}}\",\n","          \"--large_model_reference={{$.inputs.parameters[''large_model_reference'']}}\",\n","          \"--private_bucket_subdir={{$.pipeline_task_name}}_{{$.pipeline_task_uuid}}\",\n","          \"--output_dataset_path={{$.pipeline_root}}{{$.pipeline_task_name}}_{{$.pipeline_task_uuid}}\",\n","          \"--imported_data_path={{$.outputs.parameters[''imported_data_path''].output_file}}\",\n","          \"--max_num_input_examples={{$.inputs.parameters[''max_num_input_examples'']}}\",\n","          \"--executor_input={{$.json_escape[1]}}\"]}}]}, \"encryption_spec\": {\"kms_key_name\":\n","          \"{{$.inputs.parameters[''encryption_spec_key_name'']}}\"}}'\n","        - --project\n","        - '{{$.inputs.parameters[''project'']}}'\n","        - --location\n","        - '{{$.inputs.parameters[''location'']}}'\n","        - --gcp_resources\n","        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'\n","        command:\n","        - python3\n","        - -u\n","        - -m\n","        - google_cloud_pipeline_components.container.v1.custom_job.launcher\n","        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.16.1\n","    exec-private-text-importer-2:\n","      container:\n","        args:\n","        - --type\n","        - CustomJob\n","        - --payload\n","        - '{\"display_name\": \"TextImporter\", \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\":\n","          \"1\", \"machine_spec\": {\"machine_type\": \"{{$.inputs.parameters[''machine_type'']}}\"},\n","          \"container_spec\": {\"image_uri\": \"{{$.inputs.parameters[''image_uri'']}}\",\n","          \"args\": [\"--app_name=text_importer\", \"--input_text={{$.inputs.parameters[''input_text'']}}\",\n","          \"--inputs_field_name={{$.inputs.parameters[''inputs_field_name'']}}\", \"--targets_field_name={{$.inputs.parameters[''targets_field_name'']}}\",\n","          \"--output_split_name={{$.inputs.parameters[''output_split_name'']}}\", \"--instruction={{$.inputs.parameters[''instruction'']}}\",\n","          \"--large_model_reference={{$.inputs.parameters[''large_model_reference'']}}\",\n","          \"--private_bucket_subdir={{$.pipeline_task_name}}_{{$.pipeline_task_uuid}}\",\n","          \"--output_dataset_path={{$.pipeline_root}}{{$.pipeline_task_name}}_{{$.pipeline_task_uuid}}\",\n","          \"--imported_data_path={{$.outputs.parameters[''imported_data_path''].output_file}}\",\n","          \"--max_num_input_examples={{$.inputs.parameters[''max_num_input_examples'']}}\",\n","          \"--executor_input={{$.json_escape[1]}}\"]}}]}, \"encryption_spec\": {\"kms_key_name\":\n","          \"{{$.inputs.parameters[''encryption_spec_key_name'']}}\"}}'\n","        - --project\n","        - '{{$.inputs.parameters[''project'']}}'\n","        - --location\n","        - '{{$.inputs.parameters[''location'']}}'\n","        - --gcp_resources\n","        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'\n","        command:\n","        - python3\n","        - -u\n","        - -m\n","        - google_cloud_pipeline_components.container.v1.custom_job.launcher\n","        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.16.1\n","    exec-refined-upload-llm-model:\n","      container:\n","        args:\n","        - --executor_input\n","        - '{{$}}'\n","        - --function_to_execute\n","        - refined_upload_llm_model\n","        command:\n","        - sh\n","        - -ec\n","        - 'program_path=$(mktemp -d)\n","\n","\n","          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n","\n","          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n","\n","          '\n","        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\n","          \\ *\\n\\ndef refined_upload_llm_model(\\n    project: str,\\n    location: str,\\n\\\n","          \\    artifact_uri: str,\\n    model_reference_name: str,\\n    model_display_name:\\\n","          \\ str,\\n    regional_endpoint: str,\\n    model_resource_name: dsl.OutputPath(str),\\n\\\n","          \\    gcp_resources: dsl.OutputPath(str),\\n    encryption_spec_key_name:\\\n","          \\ str = '',\\n    upload_model: bool = True,\\n    tune_type: str = '',\\n\\\n","          ):\\n  \\\"\\\"\\\"Uploads LLM model.\\n\\n  Args:\\n      project: Name of the GCP\\\n","          \\ project.\\n      location: Location for model upload and deployment.\\n\\\n","          \\      artifact_uri: Path to the artifact to upload.\\n      model_reference_name:\\\n","          \\ Large model reference name.\\n      model_display_name: Name of the model\\\n","          \\ (shown in Model Registry).\\n      regional_endpoint: Regional API endpoint.\\n\\\n","          \\      encryption_spec_key_name: Customer-managed encryption key.\\n    \\\n","          \\  upload_model: Whether to upload the model to the Model Registry. Default\\n\\\n","          \\        is ``True``. If ``False``, the model will not be uploaded and output\\n\\\n","          \\        artifacts will contain empty strings.\\n      tune_type: Method\\\n","          \\ used to tune the model, e.g. ``rlhf``. If present, this\\n        value\\\n","          \\ is used to set the ``tune-type`` run label during model upload.\\n\\n  Returns:\\n\\\n","          \\      model_resource_name: Path to the created Model on Model Registry.\\n\\\n","          \\      gcp_resources: Serialized JSON of `gcp_resources`.\\n  \\\"\\\"\\\"\\n  import\\\n","          \\ json\\n  import logging\\n  import os\\n  import sys\\n\\n  try:\\n    from\\\n","          \\ google_cloud_pipeline_components.container.v1.gcp_launcher import lro_remote_runner\\n\\\n","          \\  except ImportError:\\n    from google_cloud_pipeline_components.container.v1.gcp_launcher\\\n","          \\ import lro_remote_runner\\n\\n  try:\\n    os.makedirs(os.path.dirname(model_resource_name),\\\n","          \\ exist_ok=True)\\n\\n    if not upload_model:\\n      with open(model_resource_name,\\\n","          \\ 'w') as fout:\\n        fout.write('')\\n      return\\n\\n    pipeline_labels_str\\\n","          \\ = os.getenv('VERTEX_AI_PIPELINES_RUN_LABELS')\\n    labels = json.loads(pipeline_labels_str)\\\n","          \\ if pipeline_labels_str else {}\\n    labels['google-vertex-llm-tuning-base-model-id']\\\n","          \\ = (\\n        model_reference_name.replace('@', '-')\\n    )\\n    if tune_type:\\n\\\n","          \\      labels['tune-type'] = tune_type\\n\\n    model_upload_payload = {\\n\\\n","          \\        'model': {\\n            'displayName': model_display_name,\\n  \\\n","          \\          'largeModelReference': {'name': model_reference_name},\\n    \\\n","          \\        'labels': labels,\\n            'generatedModelSource': {'genie_source':\\\n","          \\ {'base_model_uri': ''}},\\n            'artifactUri': artifact_uri,\\n \\\n","          \\       }\\n    }\\n    if encryption_spec_key_name:\\n      model_upload_payload['model']['encryption_spec']\\\n","          \\ = {\\n          'kms_key_name': encryption_spec_key_name\\n      }\\n\\n \\\n","          \\   regional_endpoint = regional_endpoint.rstrip('/')\\n    upload_model_uri\\\n","          \\ = (\\n        f'{regional_endpoint}/projects/{project}/locations/{location}/models:'\\n\\\n","          \\        'upload'\\n    )\\n\\n    remote_runner = lro_remote_runner.LroRemoteRunner(location)\\n\\\n","          \\    upload_model_lro = remote_runner.create_lro(\\n        upload_model_uri,\\n\\\n","          \\        json.dumps(model_upload_payload),\\n        gcp_resources,\\n   \\\n","          \\ )\\n    upload_model_lro = remote_runner.poll_lro(lro=upload_model_lro)\\n\\\n","          \\    model_resource = upload_model_lro['response']['model']\\n    model_version_id\\\n","          \\ = upload_model_lro['response'].get(\\n        'model_version_id'\\n    )\\\n","          \\ or upload_model_lro['response'].get('modelVersionId')\\n    if model_version_id:\\n\\\n","          \\      model_resource += f'@{model_version_id}'\\n\\n    with open(model_resource_name,\\\n","          \\ 'w') as fout:\\n      fout.write(model_resource)\\n\\n  except Exception\\\n","          \\ as e:  # pylint: disable=broad-exception-caught\\n    if isinstance(e,\\\n","          \\ ValueError):\\n      raise\\n    logging.exception(str(e))\\n    sys.exit(13)\\n\\\n","          \\n\"\n","        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.16.1\n","    exec-reinforcer:\n","      container:\n","        args:\n","        - --type\n","        - CustomJob\n","        - --payload\n","        - '{\"display_name\": \"Reinforcer\", \"job_spec\": {\"worker_pool_specs\": [{\"replica_count\":\n","          \"1\", \"machine_spec\": {\"machine_type\": \"{{$.inputs.parameters[''machine_type'']}}\",\n","          \"accelerator_type\": \"{{$.inputs.parameters[''accelerator_type'']}}\", \"accelerator_count\":\n","          {{$.inputs.parameters[''accelerator_count'']}}}, \"container_spec\": {\"image_uri\":\n","          \"{{$.inputs.parameters[''image_uri'']}}\", \"args\": [\"--app_name=reinforcer\",\n","          \"--input_reference_model_path={{$.inputs.parameters[''input_reference_model_path'']}}\",\n","          \"--input_reward_model_path={{$.inputs.parameters[''input_reward_model_path'']}}\",\n","          \"--input_reward_adapter_path={{$.inputs.parameters[''input_reward_adapter_path'']}}\",\n","          \"--input_dataset_path={{$.inputs.parameters[''input_dataset_path'']}}\",\n","          \"--input_preference_dataset_path={{$.inputs.parameters[''input_preference_dataset_path'']}}\",\n","          \"--train_steps={{$.inputs.parameters[''train_steps'']}}\", \"--output_model_path={{$.outputs.parameters[''output_model_path''].output_file}}\",\n","          \"--output_adapter_path={{$.outputs.parameters[''output_adapter_path''].output_file}}\",\n","          \"--tensorboard_metrics_path={{$.outputs.artifacts[''tensorboard_metrics''].path}}\",\n","          \"--large_model_reference={{$.inputs.parameters[''large_model_reference'']}}\",\n","          \"--reward_model_reference={{$.inputs.parameters[''reward_model_reference'']}}\",\n","          \"--inputs_sequence_length={{$.inputs.parameters[''inputs_sequence_length'']}}\",\n","          \"--targets_sequence_length={{$.inputs.parameters[''targets_sequence_length'']}}\",\n","          \"--train_split={{$.inputs.parameters[''train_split'']}}\", \"--batch_size={{$.inputs.parameters[''batch_size'']}}\",\n","          \"--learning_rate_multiplier={{$.inputs.parameters[''learning_rate_multiplier'']}}\",\n","          \"--kl_coeff={{$.inputs.parameters[''kl_coeff'']}}\", \"--lora_dim={{$.inputs.parameters[''lora_dim'']}}\",\n","          \"--reward_lora_dim={{$.inputs.parameters[''reward_lora_dim'']}}\", \"--num_microbatches={{$.inputs.parameters[''num_microbatches'']}}\"]}}],\n","          \"base_output_directory\": {\"output_uri_prefix\": \"{{$.outputs.artifacts[''tensorboard_metrics''].uri}}\"},\n","          \"tensorboard\": \"{{$.inputs.parameters[''tensorboard_resource_id'']}}\"},\n","          \"encryption_spec\": {\"kms_key_name\": \"{{$.inputs.parameters[''encryption_spec_key_name'']}}\"}}'\n","        - --project\n","        - '{{$.inputs.parameters[''project'']}}'\n","        - --location\n","        - '{{$.inputs.parameters[''location'']}}'\n","        - --gcp_resources\n","        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'\n","        command:\n","        - python3\n","        - -u\n","        - -m\n","        - google_cloud_pipeline_components.container.v1.custom_job.launcher\n","        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.16.1\n","    exec-reward-model-trainer:\n","      container:\n","        args:\n","        - --type\n","        - CustomJob\n","        - --payload\n","        - '{\"display_name\": \"RewardModelTrainer\", \"job_spec\": {\"worker_pool_specs\":\n","          [{\"replica_count\": \"1\", \"machine_spec\": {\"machine_type\": \"{{$.inputs.parameters[''machine_type'']}}\",\n","          \"accelerator_type\": \"{{$.inputs.parameters[''accelerator_type'']}}\", \"accelerator_count\":\n","          {{$.inputs.parameters[''accelerator_count'']}}}, \"container_spec\": {\"image_uri\":\n","          \"{{$.inputs.parameters[''image_uri'']}}\", \"args\": [\"--app_name=reward_model_trainer\",\n","          \"--train_steps={{$.inputs.parameters[''train_steps'']}}\", \"--input_model_path={{$.inputs.parameters[''input_model_path'']}}\",\n","          \"--input_dataset_path={{$.inputs.parameters[''input_dataset_path'']}}\",\n","          \"--eval_dataset_path={{$.inputs.parameters[''eval_dataset_path'']}}\", \"--output_adapter_path={{$.outputs.parameters[''output_adapter_path''].output_file}}\",\n","          \"--tensorboard_metrics_path={{$.outputs.artifacts[''tensorboard_metrics''].path}}\",\n","          \"--large_model_reference={{$.inputs.parameters[''large_model_reference'']}}\",\n","          \"--inputs_sequence_length={{$.inputs.parameters[''inputs_sequence_length'']}}\",\n","          \"--targets_sequence_length={{$.inputs.parameters[''targets_sequence_length'']}}\",\n","          \"--train_split={{$.inputs.parameters[''train_split'']}}\", \"--batch_size={{$.inputs.parameters[''batch_size'']}}\",\n","          \"--learning_rate_multiplier={{$.inputs.parameters[''learning_rate_multiplier'']}}\",\n","          \"--lora_dim={{$.inputs.parameters[''lora_dim'']}}\", \"--num_microbatches={{$.inputs.parameters[''num_microbatches'']}}\"]}}],\n","          \"base_output_directory\": {\"output_uri_prefix\": \"{{$.outputs.artifacts[''tensorboard_metrics''].uri}}\"},\n","          \"tensorboard\": \"{{$.inputs.parameters[''tensorboard_resource_id'']}}\"},\n","          \"encryption_spec\": {\"kms_key_name\": \"{{$.inputs.parameters[''encryption_spec_key_name'']}}\"}}'\n","        - --project\n","        - '{{$.inputs.parameters[''project'']}}'\n","        - --location\n","        - '{{$.inputs.parameters[''location'']}}'\n","        - --gcp_resources\n","        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'\n","        command:\n","        - python3\n","        - -u\n","        - -m\n","        - google_cloud_pipeline_components.container.v1.custom_job.launcher\n","        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.16.1\n","    exec-rlhf-preprocessor:\n","      container:\n","        args:\n","        - --type\n","        - CustomJob\n","        - --payload\n","        - '{\"display_name\": \"rlhf_preprocessor\", \"job_spec\": {\"worker_pool_specs\":\n","          [{\"replica_count\": \"1\", \"machine_spec\": {\"machine_type\": \"n1-standard-4\"},\n","          \"container_spec\": {\"image_uri\": \"{{$.inputs.parameters[''image_uri'']}}\",\n","          \"args\": [\"--app_name=rlhf_preprocessor\", \"--evaluation_dataset={{$.inputs.parameters[''evaluation_dataset'']}}\",\n","          \"--tensorboard_resource_id={{$.inputs.parameters[''tensorboard_resource_id'']}}\",\n","          \"--large_model_reference={{$.inputs.parameters[''large_model_reference'']}}\",\n","          \"--input_reference_model_path={{$.inputs.parameters[''input_reference_model_path'']}}\",\n","          \"--accelerator_type={{$.inputs.parameters[''accelerator_type'']}}\", \"--use_test_spec={{$.inputs.parameters[''use_test_spec'']}}\",\n","          \"--project={{$.inputs.parameters[''project'']}}\", \"--location={{$.inputs.parameters[''location'']}}\",\n","          \"--artifact_registry={{$.inputs.parameters[''artifact_registry'']}}\", \"--tag={{$.inputs.parameters[''tag'']}}\",\n","          \"--use_experimental_image={{$.inputs.parameters[''use_experimental_image'']}}\",\n","          \"--upload_location={{$.inputs.parameters[''upload_location'']}}\", \"--deploy_model={{$.inputs.parameters[''deploy_model'']}}\",\n","          \"--model_display_name={{$.inputs.parameters[''model_display_name'']}}\",\n","          \"--has_tensorboard_id_path={{$.outputs.parameters[''has_tensorboard_id''].output_file}}\",\n","          \"--has_inference_dataset_path={{$.outputs.parameters[''has_inference_dataset''].output_file}}\",\n","          \"--metadata_candidate_columns_string_path={{$.outputs.parameters[''metadata_candidate_columns_string''].output_file}}\",\n","          \"--metadata_large_model_reference_path={{$.outputs.parameters[''metadata_large_model_reference''].output_file}}\",\n","          \"--metadata_reference_model_path_path={{$.outputs.parameters[''metadata_reference_model_path''].output_file}}\",\n","          \"--metadata_reward_model_reference_path={{$.outputs.parameters[''metadata_reward_model_reference''].output_file}}\",\n","          \"--metadata_reward_model_path_path={{$.outputs.parameters[''metadata_reward_model_path''].output_file}}\",\n","          \"--metadata_machine_type_path={{$.outputs.parameters[''metadata_machine_type''].output_file}}\",\n","          \"--metadata_tuning_location_path={{$.outputs.parameters[''metadata_tuning_location''].output_file}}\",\n","          \"--metadata_accelerator_type_path={{$.outputs.parameters[''metadata_accelerator_type''].output_file}}\",\n","          \"--metadata_accelerator_count_path={{$.outputs.parameters[''metadata_accelerator_count''].output_file}}\",\n","          \"--metadata_refined_image_uri_path={{$.outputs.parameters[''metadata_refined_image_uri''].output_file}}\",\n","          \"--metadata_num_microbatches_path={{$.outputs.parameters[''metadata_num_microbatches''].output_file}}\",\n","          \"--metadata_upload_location_path={{$.outputs.parameters[''metadata_upload_location''].output_file}}\",\n","          \"--metadata_deploy_model_path={{$.outputs.parameters[''metadata_deploy_model''].output_file}}\",\n","          \"--metadata_model_display_name_path={{$.outputs.parameters[''metadata_model_display_name''].output_file}}\",\n","          \"--metadata_upload_model_path={{$.outputs.parameters[''metadata_upload_model''].output_file}}\"]}}]}}'\n","        - --project\n","        - '{{$.pipeline_google_cloud_project_id}}'\n","        - --location\n","        - '{{$.pipeline_google_cloud_location}}'\n","        - --gcp_resources\n","        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'\n","        command:\n","        - python3\n","        - -u\n","        - -m\n","        - google_cloud_pipeline_components.container.v1.custom_job.launcher\n","        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.16.1\n","    exec-validate-pipeline:\n","      container:\n","        args:\n","        - --executor_input\n","        - '{{$}}'\n","        - --function_to_execute\n","        - validate_pipeline\n","        command:\n","        - sh\n","        - -ec\n","        - 'program_path=$(mktemp -d)\n","\n","\n","          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n","\n","          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n","\n","          '\n","        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\n","          \\ *\\n\\ndef validate_pipeline(\\n    location: str,\\n    encryption_spec_key_name:\\\n","          \\ str = '',\\n    accelerator_type: str = '',\\n    eval_dataset: Optional[str]\\\n","          \\ = None,\\n) -> NamedTuple('PreprocessedInputs', reward_model_eval_dataset=str):\\n\\\n","          \\  # fmt: off\\n  \\\"\\\"\\\"Validates and preprocesses RLHF pipeline parameters.\\n\\\n","          \\n  Args:\\n    location: Location used to run non-tuning components, i.e.\\\n","          \\ components\\n      that do not require accelerators. If not specified the\\\n","          \\ location used\\n      to run the pipeline will be used.\\n    encryption_spec_key_name:\\\n","          \\ If set, CMEK support will be validated.\\n    accelerator_type: One of\\\n","          \\ 'TPU' or 'GPU'. If 'TPU' is specified, tuning\\n      components run in\\\n","          \\ europe-west4. Otherwise tuning components run in\\n      us-central1 on\\\n","          \\ GPUs. Default is 'GPU'.\\n    eval_dataset: Optional Cloud storage path\\\n","          \\ to an evaluation dataset. The\\n      format should match that of the preference\\\n","          \\ dataset.\\n  \\\"\\\"\\\"\\n  # fmt: on\\n  # pylint: disable=g-import-not-at-top,import-outside-toplevel\\n\\\n","          \\  import json\\n  import logging\\n  import re\\n  import sys\\n  import glob\\n\\\n","          \\  # pylint: enable=g-import-not-at-top,import-outside-toplevel\\n  outputs\\\n","          \\ = NamedTuple(\\n      'PreprocessedInputs',\\n      reward_model_eval_dataset=str,\\n\\\n","          \\  )\\n\\n  try:\\n    # [ Set eval_dataset\\n    eval_dataset = eval_dataset\\\n","          \\ or ''\\n    gcs_eval_dataset_uri = re.sub('^gs://', '/gcs/', eval_dataset)\\n\\\n","          \\    files_in_folder = glob.glob(gcs_eval_dataset_uri)\\n    if not files_in_folder:\\n\\\n","          \\      eval_dataset = ''\\n    else:\\n      first_file = files_in_folder[0]\\n\\\n","          \\      required_fields = ('candidate_0', 'candidate_1', 'choice')\\n    \\\n","          \\  oneof_fields = {'input_text', 'messages'}\\n      max_lines_to_check =\\\n","          \\ 100\\n      with open(first_file, 'r') as inputs:\\n        for i, line\\\n","          \\ in enumerate(inputs):\\n          json_data = json.loads(line)\\n      \\\n","          \\    is_valid_preference_data = all(\\n              field in json_data for\\\n","          \\ field in required_fields\\n          ) and any(oneof_field in json_data\\\n","          \\ for oneof_field in oneof_fields)\\n          if not is_valid_preference_data:\\n\\\n","          \\            eval_dataset = ''\\n          if not eval_dataset or i >= max_lines_to_check:\\n\\\n","          \\            break\\n    # ]\\n    # [ Check CMEK\\n    supported_pipeline_regions\\\n","          \\ = {\\n        'asia-northeast1',\\n        'asia-northeast3',\\n        'asia-southeast1',\\n\\\n","          \\        'europe-west1',\\n        'europe-west2',\\n        'europe-west3',\\n\\\n","          \\        'europe-west4',\\n        'europe-west9',\\n        'northamerica-northeast1',\\n\\\n","          \\        'us-central1',\\n        'us-east4',\\n        'us-west1',\\n    \\\n","          \\    'us-west4',\\n    }\\n    if location not in supported_pipeline_regions:\\n\\\n","          \\      raise ValueError(\\n          f'Unsupported pipeline region: {location}.\\\n","          \\ Must be one of'\\n          f' {supported_pipeline_regions}.'\\n      )\\n\\\n","          \\n    valid_cmek_accelerator_types = {\\n        'GPU',\\n        'CPU', \\\n","          \\ # Only used for testing.\\n    }\\n    valid_cmek_config = (\\n        location\\\n","          \\ == 'us-central1'\\n        and accelerator_type in valid_cmek_accelerator_types\\n\\\n","          \\    )\\n    if encryption_spec_key_name and not valid_cmek_config:\\n   \\\n","          \\   raise ValueError(\\n          'encryption_spec_key_name (CMEK) is only\\\n","          \\ supported for GPU training'\\n          ' in us-central1. Please either\\\n","          \\ unset encryption_spec_key_name or'\\n          ' create your pipeline in\\\n","          \\ us-central1 to use GPU instead.'\\n      )\\n    # CMEK ]\\n\\n    return\\\n","          \\ outputs(reward_model_eval_dataset=eval_dataset)\\n\\n  except Exception\\\n","          \\ as e:  # pylint: disable=broad-exception-caught\\n    if isinstance(e,\\\n","          \\ ValueError):\\n      raise\\n    logging.exception(str(e))\\n    sys.exit(13)\\n\\\n","          \\n\"\n","        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.16.1\n","pipelineInfo:\n","  description: Performs reinforcement learning from human feedback.\n","  name: rlhf-train-template\n","root:\n","  dag:\n","    outputs:\n","      parameters:\n","        endpoint_resource_name:\n","          valueFromParameter:\n","            outputParameterKey: endpoint_resource_name\n","            producerSubtask: llm-deployment-graph\n","        model_resource_name:\n","          valueFromParameter:\n","            outputParameterKey: model_resource_name\n","            producerSubtask: llm-deployment-graph\n","    tasks:\n","      condition-1:\n","        componentRef:\n","          name: comp-condition-1\n","        dependentTasks:\n","        - reinforcement-learning-graph\n","        - rlhf-preprocessor\n","        inputs:\n","          parameters:\n","            pipelinechannel--accelerator_type:\n","              componentInputParameter: accelerator_type\n","            pipelinechannel--encryption_spec_key_name:\n","              componentInputParameter: encryption_spec_key_name\n","            pipelinechannel--eval_dataset:\n","              componentInputParameter: eval_dataset\n","            pipelinechannel--instruction:\n","              componentInputParameter: instruction\n","            pipelinechannel--large_model_reference:\n","              componentInputParameter: large_model_reference\n","            pipelinechannel--location:\n","              componentInputParameter: location\n","            pipelinechannel--project:\n","              componentInputParameter: project\n","            pipelinechannel--prompt_sequence_length:\n","              componentInputParameter: prompt_sequence_length\n","            pipelinechannel--reinforcement-learning-graph-output_model_path:\n","              taskOutputParameter:\n","                outputParameterKey: output_model_path\n","                producerTask: reinforcement-learning-graph\n","            pipelinechannel--rlhf-preprocessor-has_inference_dataset:\n","              taskOutputParameter:\n","                outputParameterKey: has_inference_dataset\n","                producerTask: rlhf-preprocessor\n","            pipelinechannel--target_sequence_length:\n","              componentInputParameter: target_sequence_length\n","        taskInfo:\n","          name: Perform Inference\n","        triggerPolicy:\n","          condition: inputs.parameter_values['pipelinechannel--rlhf-preprocessor-has_inference_dataset']\n","            == true\n","      llm-deployment-graph:\n","        cachingOptions:\n","          enableCache: true\n","        componentRef:\n","          name: comp-llm-deployment-graph\n","        dependentTasks:\n","        - reinforcement-learning-graph\n","        - rlhf-preprocessor\n","        inputs:\n","          parameters:\n","            deploy_model:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_deploy_model\n","                producerTask: rlhf-preprocessor\n","            encryption_spec_key_name:\n","              componentInputParameter: encryption_spec_key_name\n","            large_model_reference:\n","              componentInputParameter: large_model_reference\n","            model_display_name:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_model_display_name\n","                producerTask: rlhf-preprocessor\n","            output_adapter_path:\n","              taskOutputParameter:\n","                outputParameterKey: output_adapter_path\n","                producerTask: reinforcement-learning-graph\n","            policy_model_reference:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_large_model_reference\n","                producerTask: rlhf-preprocessor\n","            regional_endpoint:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_upload_location\n","                producerTask: rlhf-preprocessor\n","            upload_location:\n","              componentInputParameter: location\n","            upload_model:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_upload_model\n","                producerTask: rlhf-preprocessor\n","        taskInfo:\n","          name: Upload and Deploy Tuned Model\n","      reinforcement-learning-graph:\n","        cachingOptions:\n","          enableCache: true\n","        componentRef:\n","          name: comp-reinforcement-learning-graph\n","        dependentTasks:\n","        - reward-model-graph\n","        - rlhf-preprocessor\n","        inputs:\n","          parameters:\n","            accelerator_count:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_accelerator_count\n","                producerTask: rlhf-preprocessor\n","            accelerator_type:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_accelerator_type\n","                producerTask: rlhf-preprocessor\n","            encryption_spec_key_name:\n","              componentInputParameter: encryption_spec_key_name\n","            input_preference_dataset_path:\n","              taskOutputParameter:\n","                outputParameterKey: reward_dataset_path\n","                producerTask: reward-model-graph\n","            input_reward_adapter_path:\n","              taskOutputParameter:\n","                outputParameterKey: reward_model_adapter_path\n","                producerTask: reward-model-graph\n","            input_reward_model_path:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_reward_model_path\n","                producerTask: rlhf-preprocessor\n","            instruction:\n","              componentInputParameter: instruction\n","            kl_coeff:\n","              componentInputParameter: kl_coeff\n","            large_model_reference:\n","              componentInputParameter: large_model_reference\n","            location:\n","              componentInputParameter: location\n","            machine_type:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_machine_type\n","                producerTask: rlhf-preprocessor\n","            num_microbatches:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_num_microbatches\n","                producerTask: rlhf-preprocessor\n","            policy_model_path:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_reference_model_path\n","                producerTask: rlhf-preprocessor\n","            policy_model_reference:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_large_model_reference\n","                producerTask: rlhf-preprocessor\n","            project:\n","              componentInputParameter: project\n","            prompt_dataset:\n","              componentInputParameter: prompt_dataset\n","            prompt_sequence_length:\n","              componentInputParameter: prompt_sequence_length\n","            reinforcement_learning_rate_multiplier:\n","              componentInputParameter: reinforcement_learning_rate_multiplier\n","            reinforcement_learning_train_steps:\n","              componentInputParameter: reinforcement_learning_train_steps\n","            reward_lora_dim:\n","              runtimeValue:\n","                constant: 4.0\n","            reward_model_reference:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_reward_model_reference\n","                producerTask: rlhf-preprocessor\n","            rl_image_uri:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_refined_image_uri\n","                producerTask: rlhf-preprocessor\n","            target_sequence_length:\n","              componentInputParameter: target_sequence_length\n","            tensorboard_resource_id:\n","              componentInputParameter: tensorboard_resource_id\n","            tuning_location:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_tuning_location\n","                producerTask: rlhf-preprocessor\n","        taskInfo:\n","          name: Reinforcement Learning\n","      reward-model-graph:\n","        cachingOptions:\n","          enableCache: true\n","        componentRef:\n","          name: comp-reward-model-graph\n","        dependentTasks:\n","        - rlhf-preprocessor\n","        - validate-pipeline\n","        inputs:\n","          parameters:\n","            accelerator_count:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_accelerator_count\n","                producerTask: rlhf-preprocessor\n","            accelerator_type:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_accelerator_type\n","                producerTask: rlhf-preprocessor\n","            comma_separated_candidates_field_names:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_candidate_columns_string\n","                producerTask: rlhf-preprocessor\n","            encryption_spec_key_name:\n","              componentInputParameter: encryption_spec_key_name\n","            eval_dataset:\n","              taskOutputParameter:\n","                outputParameterKey: reward_model_eval_dataset\n","                producerTask: validate-pipeline\n","            instruction:\n","              componentInputParameter: instruction\n","            large_model_reference:\n","              componentInputParameter: large_model_reference\n","            location:\n","              componentInputParameter: location\n","            lora_dim:\n","              runtimeValue:\n","                constant: 4.0\n","            machine_type:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_machine_type\n","                producerTask: rlhf-preprocessor\n","            num_microbatches:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_num_microbatches\n","                producerTask: rlhf-preprocessor\n","            preference_dataset:\n","              componentInputParameter: preference_dataset\n","            project:\n","              componentInputParameter: project\n","            prompt_sequence_length:\n","              componentInputParameter: prompt_sequence_length\n","            reward_model_image_uri:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_refined_image_uri\n","                producerTask: rlhf-preprocessor\n","            reward_model_learning_rate_multiplier:\n","              componentInputParameter: reward_model_learning_rate_multiplier\n","            reward_model_path:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_reward_model_path\n","                producerTask: rlhf-preprocessor\n","            reward_model_reference:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_reward_model_reference\n","                producerTask: rlhf-preprocessor\n","            reward_model_train_steps:\n","              componentInputParameter: reward_model_train_steps\n","            target_sequence_length:\n","              componentInputParameter: target_sequence_length\n","            tensorboard_resource_id:\n","              componentInputParameter: tensorboard_resource_id\n","            tuning_location:\n","              taskOutputParameter:\n","                outputParameterKey: metadata_tuning_location\n","                producerTask: rlhf-preprocessor\n","        taskInfo:\n","          name: Train Reward Model\n","      rlhf-preprocessor:\n","        cachingOptions:\n","          enableCache: true\n","        componentRef:\n","          name: comp-rlhf-preprocessor\n","        inputs:\n","          parameters:\n","            accelerator_type:\n","              componentInputParameter: accelerator_type\n","            artifact_registry:\n","              runtimeValue:\n","                constant: rlhf\n","            deploy_model:\n","              componentInputParameter: deploy_model\n","            evaluation_dataset:\n","              componentInputParameter: eval_dataset\n","            large_model_reference:\n","              componentInputParameter: large_model_reference\n","            location:\n","              runtimeValue:\n","                constant: us\n","            model_display_name:\n","              componentInputParameter: model_display_name\n","            project:\n","              runtimeValue:\n","                constant: vertex-ai-restricted\n","            tag:\n","              runtimeValue:\n","                constant: '20240623_1707'\n","            tensorboard_resource_id:\n","              componentInputParameter: tensorboard_resource_id\n","            upload_location:\n","              componentInputParameter: location\n","            use_test_spec:\n","              runtimeValue:\n","                constant: false\n","        taskInfo:\n","          name: Preprocess Inputs\n","      validate-pipeline:\n","        cachingOptions:\n","          enableCache: true\n","        componentRef:\n","          name: comp-validate-pipeline\n","        inputs:\n","          parameters:\n","            accelerator_type:\n","              componentInputParameter: accelerator_type\n","            encryption_spec_key_name:\n","              componentInputParameter: encryption_spec_key_name\n","            eval_dataset:\n","              componentInputParameter: eval_dataset\n","            location:\n","              componentInputParameter: location\n","        taskInfo:\n","          name: Validate Inputs\n","  inputDefinitions:\n","    parameters:\n","      accelerator_type:\n","        defaultValue: GPU\n","        description: One of 'TPU' or 'GPU'. If 'TPU' is specified, tuning components\n","          run in europe-west4. Otherwise tuning components run in us-central1 on GPUs.\n","          Default is 'GPU'.\n","        isOptional: true\n","        parameterType: STRING\n","      deploy_model:\n","        defaultValue: true\n","        description: Whether to deploy the model to an endpoint in `us-central1`.\n","          Default is True.\n","        isOptional: true\n","        parameterType: BOOLEAN\n","      encryption_spec_key_name:\n","        defaultValue: ''\n","        description: Customer-managed encryption key. If this is set, then all resources\n","          created by the CustomJob will be encrypted with the provided encryption\n","          key. Note that this is not supported for TPU at the moment.\n","        isOptional: true\n","        parameterType: STRING\n","      eval_dataset:\n","        description: Optional Cloud storage path to an evaluation dataset. The dataset\n","          format is jsonl. The evaluation dataset can be used to compute train-time\n","          metrics (when training a reward model) or perform bulk inference for third-party\n","          models. To compute train-time metrics this dataset must contain the same\n","          fields as the peference dataset. For bulk inference with third-party models\n","          only `input_text` is needed. Note, train-time metrics are only computed\n","          for the first 5000 samples in the dataset for efficient evaluation during\n","          training.\n","        isOptional: true\n","        parameterType: STRING\n","      instruction:\n","        description: This field lets the model know what task it needs to perform.\n","          Base models have been trained over a large set of varied instructions. You\n","          can give a simple and intuitive description of the task and the model will\n","          follow it, e.g. \"Classify this movie review as positive or negative\" or\n","          \"Translate this sentence to Danish\". Do not specify this if your dataset\n","          already prepends the instruction to the inputs field.\n","        isOptional: true\n","        parameterType: STRING\n","      kl_coeff:\n","        defaultValue: 0.1\n","        description: Coefficient for KL penalty. This regularizes the policy model\n","          and penalizes if it diverges from its initial distribution. If set to 0,\n","          the reference language model is not loaded into memory. Default value is\n","          0.1.\n","        isOptional: true\n","        parameterType: NUMBER_DOUBLE\n","      large_model_reference:\n","        description: Name of the base model. Supported values are `text-bison@001`,\n","          `t5-small`, `t5-large`, `t5-xl` and `t5-xxl`. `text-bison@001` and `t5-small`\n","          are supported in `us-central1` and `europe-west4`. `t5-large`, `t5-xl` and\n","          `t5-xxl` are only supported in `europe-west4`.\n","        parameterType: STRING\n","      location:\n","        defaultValue: '{{$.pipeline_google_cloud_location}}'\n","        description: Location used to run non-tuning components, i.e. components that\n","          do not require accelerators. If not specified the location used to run the\n","          pipeline will be used.\n","        isOptional: true\n","        parameterType: STRING\n","      model_display_name:\n","        description: Name of the fine-tuned model shown in the Model Registry. If\n","          not provided, a default name will be created.\n","        isOptional: true\n","        parameterType: STRING\n","      preference_dataset:\n","        description: Cloud storage path to a human preference JSONL dataset used to\n","          train a reward model. Each example in a preference dataset must contain\n","          `candidate_0` and `candidate_1` fields that contain candidate responses,\n","          `choice` that specifies the preferred candidate and either `input_text`\n","          (if tuning a text model) or `messages` (if tuning a chat model). Chat datasets\n","          must contain at least 1 message in a `messages` field. Each message must\n","          be valid JSON that contains `author` and `content` fields, where valid `author`\n","          values are `user` and `assistant` and `content` must be non-empty. Each\n","          row may contain multiple messages, but the first and last author must be\n","          the `user`. An optional `context` field may be provided for each example\n","          in a chat dataset. If provided, the `context` will preprended to the message\n","          `content`. The `instruction` serves as the default context. (Useful if most\n","          messages use the same system-level context.) Any context provided in the\n","          example will override the default value.\n","        parameterType: STRING\n","      project:\n","        defaultValue: '{{$.pipeline_google_cloud_project_id}}'\n","        description: Project used to run custom jobs. If not specified the project\n","          used to run the pipeline will be used.\n","        isOptional: true\n","        parameterType: STRING\n","      prompt_dataset:\n","        description: Cloud storage path to an unlabled JSONL dataset that contains\n","          prompts. Text datasets must contain an `input_text` field that contains\n","          the prompt. Chat datasets must contain at least 1 message in a `messages`\n","          field. Each message must be valid JSON that contains `author` and `content`\n","          fields, where valid `author` values are `user` and `assistant` and `content`\n","          must be non-empty. Each row may contain multiple messages, but the first\n","          and last author must be the `user`. An optional `context` field may be provided\n","          for each example in a chat dataset. If provided, the `context` will preprended\n","          to the message `content`. The `instruction` serves as the default context.\n","          (Useful if most messages use the same system-level context.) Any context\n","          provided in the example will override the default value.\n","        parameterType: STRING\n","      prompt_sequence_length:\n","        defaultValue: 512.0\n","        description: Maximum tokenized sequence length for input text. Higher values\n","          increase memory overhead. This value should be at most 8192. Default value\n","          is 512.\n","        isOptional: true\n","        parameterType: NUMBER_INTEGER\n","      reinforcement_learning_rate_multiplier:\n","        defaultValue: 1.0\n","        description: Constant used to adjust the base learning rate used during reinforcement\n","          learning. Multiply by a number > 1 to increase the magnitude of updates\n","          applied at each training step or multiply by a number < 1 to decrease the\n","          magnitude of updates. Default value is 1.0.\n","        isOptional: true\n","        parameterType: NUMBER_DOUBLE\n","      reinforcement_learning_train_steps:\n","        defaultValue: 1000.0\n","        description: Number of reinforcement learning steps to perform when tuning\n","          a base model. Default value is 1000.\n","        isOptional: true\n","        parameterType: NUMBER_INTEGER\n","      reward_model_learning_rate_multiplier:\n","        defaultValue: 1.0\n","        description: Constant used to adjust the base learning rate used when training\n","          a reward model. Multiply by a number > 1 to increase the magnitude of updates\n","          applied at each training step or multiply by a number < 1 to decrease the\n","          magnitude of updates. Default value is 1.0.\n","        isOptional: true\n","        parameterType: NUMBER_DOUBLE\n","      reward_model_train_steps:\n","        defaultValue: 1000.0\n","        description: Number of steps to use when training a reward model. Default\n","          value is 1000.\n","        isOptional: true\n","        parameterType: NUMBER_INTEGER\n","      target_sequence_length:\n","        defaultValue: 64.0\n","        description: ' Maximum tokenized sequence length for target text. Higher values\n","          increase memory overhead. This value should be at most 1024. Default value\n","          is 64.'\n","        isOptional: true\n","        parameterType: NUMBER_INTEGER\n","      tensorboard_resource_id:\n","        defaultValue: ''\n","        description: Optional tensorboard resource id in format `projects/{project_number}/locations/{location}/tensorboards/{tensorboard_id}`.\n","          If provided, tensorboard metrics will be uploaded to this location.\n","        isOptional: true\n","        parameterType: STRING\n","  outputDefinitions:\n","    parameters:\n","      endpoint_resource_name:\n","        description: Path the Online Prediction Endpoint. This will be an empty string\n","          if the model was not deployed.\n","        parameterType: STRING\n","      model_resource_name:\n","        description: Path to the model uploaded to the Model Registry. This will be\n","          an empty string if the model was not deployed.\n","        parameterType: STRING\n","schemaVersion: 2.1.0\n","sdkVersion: kfp-2.7.0\n"]}]},{"cell_type":"code","source":["# Preference dataset size\n","PREF_DATASET_SIZE = 3000"],"metadata":{"id":"k_nMvnTLK5XX","executionInfo":{"status":"ok","timestamp":1725556302948,"user_tz":-330,"elapsed":534,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Batch size is fixed at 64\n","BATCH_SIZE = 64"],"metadata":{"id":"23JSAiedK7te","executionInfo":{"status":"ok","timestamp":1725556312771,"user_tz":-330,"elapsed":508,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import math"],"metadata":{"id":"CniM11gjK9am","executionInfo":{"status":"ok","timestamp":1725556323702,"user_tz":-330,"elapsed":463,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["REWARD_STEPS_PER_EPOCH = math.ceil(PREF_DATASET_SIZE / BATCH_SIZE)\n","print(REWARD_STEPS_PER_EPOCH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R4Y_i7OzLAow","executionInfo":{"status":"ok","timestamp":1725556332920,"user_tz":-330,"elapsed":492,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}},"outputId":"6f4aace0-21ad-4395-e4b0-d002931fc24b"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["47\n"]}]},{"cell_type":"code","source":["REWARD_NUM_EPOCHS = 30"],"metadata":{"id":"JKDt3ljcLDo2","executionInfo":{"status":"ok","timestamp":1725556344777,"user_tz":-330,"elapsed":483,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Calculate number of steps in the reward model training\n","reward_model_train_steps = REWARD_STEPS_PER_EPOCH * REWARD_NUM_EPOCHS"],"metadata":{"id":"lfFHpheuLGkI","executionInfo":{"status":"ok","timestamp":1725556357000,"user_tz":-330,"elapsed":5,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["print(reward_model_train_steps)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_xcRkrykLI4F","executionInfo":{"status":"ok","timestamp":1725556366331,"user_tz":-330,"elapsed":518,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}},"outputId":"6e3523ac-18df-4bcd-c1af-246c7cdcc416"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["1410\n"]}]},{"cell_type":"markdown","source":["# Number of reinforcement learning training steps"],"metadata":{"id":"iXCphxQKLO4c"}},{"cell_type":"code","source":["# Prompt dataset size\n","PROMPT_DATASET_SIZE = 2000"],"metadata":{"id":"ES81rY7bLUbI","executionInfo":{"status":"ok","timestamp":1725556420111,"user_tz":-330,"elapsed":451,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Batch size is fixed at 64\n","BATCH_SIZE = 64"],"metadata":{"id":"PmtaeKo-LYkT","executionInfo":{"status":"ok","timestamp":1725556430632,"user_tz":-330,"elapsed":537,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["import math\n","RL_STEPS_PER_EPOCH = math.ceil(PROMPT_DATASET_SIZE / BATCH_SIZE)\n","print(RL_STEPS_PER_EPOCH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITbIA3cnLZm2","executionInfo":{"status":"ok","timestamp":1725556451300,"user_tz":-330,"elapsed":1136,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}},"outputId":"5af0f143-066e-4228-ece0-a62c2635bbdc"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["32\n"]}]},{"cell_type":"code","source":["RL_NUM_EPOCHS = 10"],"metadata":{"id":"QSMlbzLsLfqr","executionInfo":{"status":"ok","timestamp":1725556463500,"user_tz":-330,"elapsed":528,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Calculate the number of steps in the RL training\n","reinforcement_learning_train_steps = RL_STEPS_PER_EPOCH * RL_NUM_EPOCHS"],"metadata":{"id":"FqBxEYbKLjZ_","executionInfo":{"status":"ok","timestamp":1725556475267,"user_tz":-330,"elapsed":484,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["print(reinforcement_learning_train_steps)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QzI7RnssLmnH","executionInfo":{"status":"ok","timestamp":1725556489357,"user_tz":-330,"elapsed":504,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}},"outputId":"ee93c4b3-2cb1-40cb-c09b-87636e5b2a52"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["320\n"]}]},{"cell_type":"code","source":["# Completed values for the dictionary\n","parameter_values={\n","        \"preference_dataset\": \\\n","    \"gs://vertex-ai/generative-ai/rlhf/text_small/summarize_from_feedback_tfds/comparisons/train/*.jsonl\",\n","        \"prompt_dataset\": \\\n","    \"gs://vertex-ai/generative-ai/rlhf/text_small/reddit_tfds/train/*.jsonl\",\n","        \"eval_dataset\": \\\n","    \"gs://vertex-ai/generative-ai/rlhf/text_small/reddit_tfds/val/*.jsonl\",\n","        \"large_model_reference\": \"llama-2-7b\",\n","        \"reward_model_train_steps\": 1410,\n","        \"reinforcement_learning_train_steps\": 320, # results from the calculations above\n","        \"reward_model_learning_rate_multiplier\": 1.0,\n","        \"reinforcement_learning_rate_multiplier\": 1.0,\n","        \"kl_coeff\": 0.1, # increased to reduce reward hacking\n","        \"instruction\":\\\n","    \"Summarize in less than 50 words\"}"],"metadata":{"id":"nlf98kksKPBn","executionInfo":{"status":"ok","timestamp":1725556282082,"user_tz":-330,"elapsed":453,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["parameter_values={\n","        \"preference_dataset\": \\\n","    \"gs://vertex-ai/generative-ai/rlhf/text/summarize_from_feedback_tfds/comparisons/train/*.jsonl\",\n","        \"prompt_dataset\": \\\n","    \"gs://vertex-ai/generative-ai/rlhf/text/reddit_tfds/train/*.jsonl\",\n","        \"eval_dataset\": \\\n","    \"gs://vertex-ai/generative-ai/rlhf/text/reddit_tfds/val/*.jsonl\",\n","        \"large_model_reference\": \"llama-2-7b\",\n","        \"reward_model_train_steps\": 10000,\n","        \"reinforcement_learning_train_steps\": 10000,\n","        \"reward_model_learning_rate_multiplier\": 1.0,\n","        \"reinforcement_learning_rate_multiplier\": 0.2,\n","        \"kl_coeff\": 0.1,\n","        \"instruction\":\\\n","    \"Summarize in less than 50 words\"}"],"metadata":{"id":"Vkgn-RzGKVQ_","executionInfo":{"status":"ok","timestamp":1725556537010,"user_tz":-330,"elapsed":535,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Authenticate in utils\n","\n","credentials, PROJECT_ID, STAGING_BUCKET = authenticate()\n","\n","# RLFH pipeline is available in this region\n","REGION = \"europe-west4\""],"metadata":{"id":"4XRGM01JLzHo","executionInfo":{"status":"ok","timestamp":1725558062037,"user_tz":-330,"elapsed":534,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["## Run the pipeline job on Vertex AI"],"metadata":{"id":"Utlm21iuRz_h"}},{"cell_type":"code","source":["import google.cloud.aiplatform as aiplatform"],"metadata":{"id":"TcYBJWQeRj_-","executionInfo":{"status":"ok","timestamp":1725558126230,"user_tz":-330,"elapsed":3870,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["aiplatform.init(project = PROJECT_ID,\n","                location = REGION,\n","                credentials = credentials)"],"metadata":{"id":"bqqgKjUvR2S4","executionInfo":{"status":"ok","timestamp":1725558133487,"user_tz":-330,"elapsed":461,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# Look at the path for the YAML file\n","RLHF_PIPELINE_PKG_PATH"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"6Fa_drXPR457","executionInfo":{"status":"ok","timestamp":1725558142584,"user_tz":-330,"elapsed":519,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}},"outputId":"cc5d839d-bc47-4884-a961-53f705ade27a"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'rlhf_pipeline.yaml'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["job = aiplatform.PipelineJob(\n","    display_name=\"tutorial-rlhf-tuning\",\n","    pipeline_root=STAGING_BUCKET,\n","    template_path=RLHF_PIPELINE_PKG_PATH,\n","    parameter_values=parameter_values)"],"metadata":{"id":"C3AQSsf7R7HJ","executionInfo":{"status":"ok","timestamp":1725558183692,"user_tz":-330,"elapsed":483,"user":{"displayName":"SAURAB MISHRA IMS23323","userId":"16897900852261678834"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["job.run()"],"metadata":{"id":"6jF2YCcMR_pr"},"execution_count":null,"outputs":[]}]}